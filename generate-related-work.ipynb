{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Automated Literature Review Generation using Agentic RAG\n",
    "\n",
    "This notebook demonstrates an end-to-end pipeline for automatically generating \"Related Work\" sections for scientific papers using:\n",
    "- **Hybrid Retrieval**: Combining semantic search (vector embeddings) and keyword search (BM25)\n",
    "- **Agentic Relevance Scoring**: Using LLM agents to evaluate paper relevance with structured reasoning\n",
    "- **Automated Synthesis**: Generating coherent literature reviews with proper citations\n",
    "\n",
    "## Pipeline Overview\n",
    "1. Load biomedical abstracts corpus\n",
    "2. Create vector store with hybrid retrieval capabilities\n",
    "3. Define research query/abstract\n",
    "4. Retrieve candidate papers using hybrid search\n",
    "5. Score papers using debate-style relevance agent\n",
    "6. Select top-k most relevant papers\n",
    "7. Generate cohesive \"Related Work\" section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Configuration\n",
    "\n",
    "Import dependencies and configure pipeline parameters. Adjust these settings to customize the pipeline behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "autoreload",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Auto-reload modules for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import asyncio\n",
    "from typing import List, Dict, Any\n",
    "from pprint import pprint\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# OpenAI and agents\n",
    "import openai\n",
    "from agents import Agent, Runner\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated\n",
    "\n",
    "# Environment and display\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, HTML\n",
    "\n",
    "# Local modules\n",
    "from vectorstore import VectorStoreAbstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "configuration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n",
      "  - Retrieval: Top 50 papers using hybrid search\n",
      "  - Scoring: 10 abstracts will be scored\n",
      "  - Selection: Top 10 papers for related work\n",
      "  - Models: gpt-4o-mini (scoring), gpt-4o-mini (generation)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Vector Store Configuration\n",
    "CHROMA_PERSIST_DIRECTORY = \"./corpus-data/chroma_db\"\n",
    "RECREATE_INDEX = False  # Set to True to rebuild the index from scratch\n",
    "\n",
    "# Retrieval Configuration\n",
    "HYBRID_SEARCH_K = 50  # Number of papers to retrieve using hybrid search\n",
    "\n",
    "# Relevance Scoring Configuration\n",
    "NUM_ABSTRACTS_TO_SCORE = 10 # Set to None to score all retrieved abstracts, or set a number for testing (e.g., 5, 10, 20)\n",
    "RELEVANCE_MODEL = \"gpt-4o-mini\"  # Model for relevance scoring agent\n",
    "\n",
    "# Top-K Selection Configuration\n",
    "TOP_K_PAPERS = 10  # Number of top-ranked papers to include in related work\n",
    "\n",
    "# Related Work Generation Configuration\n",
    "GENERATION_MODEL = \"gpt-4o-mini\"  # Model for generating related work section\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"  - Retrieval: Top {HYBRID_SEARCH_K} papers using hybrid search\")\n",
    "print(f\"  - Scoring: {'All' if NUM_ABSTRACTS_TO_SCORE is None else NUM_ABSTRACTS_TO_SCORE} abstracts will be scored\")\n",
    "print(f\"  - Selection: Top {TOP_K_PAPERS} papers for related work\")\n",
    "print(f\"  - Models: {RELEVANCE_MODEL} (scoring), {GENERATION_MODEL} (generation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "env-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment loaded\n",
      "✓ OpenAI client initialized\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables and initialize OpenAI client\n",
    "load_dotenv(override=True)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_client = openai.OpenAI()\n",
    "\n",
    "print(\"✓ Environment loaded\")\n",
    "print(\"✓ OpenAI client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Loading & Preparation\n",
    "\n",
    "Load the biomedical abstracts corpus and prepare it for indexing. Each abstract contains:\n",
    "- **id**: Unique identifier\n",
    "- **title**: Paper title\n",
    "- **abstract**: Paper abstract\n",
    "- **title_abstract**: Concatenated title and abstract for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78 abstracts from corpus\n",
      "\n",
      "Dataset columns: ['id', 'title', 'abstract']\n",
      "\n",
      "First few abstracts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>Reshaping Biomedical Scientific Literature in ...</td>\n",
       "      <td>Biomedical Question Answering (BQA) poses spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PaperQA: Retrieval-Augmented Generative Agent ...</td>\n",
       "      <td>Large Language Models (LLMs) generalize well a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>Attention is all you need, A</td>\n",
       "      <td>The dominant sequence transduction models are ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                              title  \\\n",
       "33  34  Reshaping Biomedical Scientific Literature in ...   \n",
       "0    1  PaperQA: Retrieval-Augmented Generative Agent ...   \n",
       "34  35                       Attention is all you need, A   \n",
       "\n",
       "                                             abstract  \n",
       "33  Biomedical Question Answering (BQA) poses spec...  \n",
       "0   Large Language Models (LLMs) generalize well a...  \n",
       "34  The dominant sequence transduction models are ...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load abstracts from CSV and shuffle\n",
    "all_abstracts = pd.read_csv('./rag.csv').sample(frac=1, random_state=42)\n",
    "\n",
    "# Set to True to delete existing database\n",
    "RECREATE_INDEX = False\n",
    "\n",
    "print(f\"Loaded {len(all_abstracts)} abstracts from corpus\")\n",
    "print(f\"\\nDataset columns: {list(all_abstracts.columns)}\")\n",
    "print(f\"\\nFirst few abstracts:\")\n",
    "all_abstracts.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "prepare-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prepared 78 abstracts for indexing\n",
      "\n",
      "Sample abstract structure:\n",
      "  - ID: 34\n",
      "  - Text length: 1325 characters\n"
     ]
    }
   ],
   "source": [
    "# Concatenate title and abstract for better retrieval\n",
    "all_abstracts['title_abstract'] = all_abstracts['title'] + all_abstracts['abstract']\n",
    "\n",
    "# Convert to list of dictionaries for vector store\n",
    "samples_abstracts = [\n",
    "    v for k, v in all_abstracts[['title_abstract', 'id']].reset_index(drop=True).T.to_dict().items()\n",
    "]\n",
    "\n",
    "print(f\"✓ Prepared {len(samples_abstracts)} abstracts for indexing\")\n",
    "print(f\"\\nSample abstract structure:\")\n",
    "print(f\"  - ID: {samples_abstracts[0]['id']}\")\n",
    "print(f\"  - Text length: {len(samples_abstracts[0]['title_abstract'])} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vectorstore-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Vector Store Initialization\n",
    "\n",
    "Initialize ChromaDB vector store with hybrid retrieval capabilities:\n",
    "- **Semantic Search**: Uses HuggingFace embeddings (all-MiniLM-L6-v2)\n",
    "- **Keyword Search**: Uses BM25 algorithm\n",
    "- **Chunking**: Splits abstracts into 150-character chunks with 20-character overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "init-vectorstore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing index at ./corpus-data/chroma_db\n",
      "   Set recreate_index=True to force recreation\n",
      "✓ Using existing index at ./corpus-data/chroma_db\n",
      "  Index contains 1138 document chunks\n"
     ]
    }
   ],
   "source": [
    "# Initialize vector store\n",
    "vector_store = VectorStoreAbstract(\n",
    "    abstracts=samples_abstracts,\n",
    "    persist_directory=CHROMA_PERSIST_DIRECTORY,\n",
    "    recreate_index=RECREATE_INDEX\n",
    ")\n",
    "\n",
    "# Display index status\n",
    "if vector_store.index_exists:\n",
    "    doc_count = vector_store.get_document_count()\n",
    "    print(f\"✓ Using existing index at {CHROMA_PERSIST_DIRECTORY}\")\n",
    "    print(f\"  Index contains {doc_count} document chunks\")\n",
    "else:\n",
    "    print(f\"✓ Created new index at {CHROMA_PERSIST_DIRECTORY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "chunk-documents",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Skipping document chunking (using existing index)\n",
      "CPU times: user 211 μs, sys: 253 μs, total: 464 μs\n",
      "Wall time: 345 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Chunk documents if needed (only when creating new index or recreating)\n",
    "if vector_store.should_process_documents():\n",
    "    print(\"Chunking documents...\")\n",
    "    documents = vector_store.chunking()\n",
    "    print(f\"✓ Created {len(documents)} document chunks\")\n",
    "else:\n",
    "    print(\"✓ Skipping document chunking (using existing index)\")\n",
    "    documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "index-documents",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Skipping document indexing (using existing index)\n",
      "  Ready to perform searches!\n",
      "  Index contains 1138 chunks\n",
      "CPU times: user 506 μs, sys: 218 μs, total: 724 μs\n",
      "Wall time: 654 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Index documents if needed\n",
    "if vector_store.should_process_documents():\n",
    "    print(f\"Indexing {len(documents)} documents (this may take several minutes)...\")\n",
    "    vector_store.index_document(documents)\n",
    "    print(\"✓ Indexing completed!\")\n",
    "    print(f\"  Total chunks indexed: {vector_store.get_document_count()}\")\n",
    "else:\n",
    "    print(\"✓ Skipping document indexing (using existing index)\")\n",
    "    print(f\"  Ready to perform searches!\")\n",
    "    print(f\"  Index contains {vector_store.get_document_count()} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Research Query Definition\n",
    "\n",
    "Define the research query or abstract for which we want to generate a literature review. This will be used to:\n",
    "1. Retrieve relevant papers from the corpus\n",
    "2. Score the relevance of each retrieved paper\n",
    "3. Generate the final \"Related Work\" section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "define-query",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Research Query/Abstract"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "_Retrieval-augmented generation (RAG) systems are emerging as effective tools for biomedical literature. \n",
       "However, their performance in this domain is not yet generalizable. \n",
       "We propose a new strategy for high-performing RAG applied to biomedical question answering. \n",
       "This approach would allow the wider public and public health professionals to access evidence from scientific literature in easy-to-understand language._"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query length: 419 characters\n"
     ]
    }
   ],
   "source": [
    "# Define the research query/abstract\n",
    "query = \"\"\"\n",
    "Retrieval-augmented generation (RAG) systems are emerging as effective tools for biomedical literature. \n",
    "However, their performance in this domain is not yet generalizable. \n",
    "We propose a new strategy for high-performing RAG applied to biomedical question answering. \n",
    "This approach would allow the wider public and public health professionals to access evidence from scientific literature in easy-to-understand language.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Display the query\n",
    "display(Markdown(\"### Research Query/Abstract\"))\n",
    "display(Markdown(f\"_{query}_\"))\n",
    "print(f\"\\nQuery length: {len(query)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retrieval-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Hybrid Retrieval\n",
    "\n",
    "Perform hybrid search combining:\n",
    "- **Semantic similarity**: Vector search using embeddings\n",
    "- **Keyword matching**: BM25 ranking\n",
    "\n",
    "The ensemble retriever combines both methods with equal weights (0.5, 0.5) to balance semantic understanding and keyword relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "hybrid-search",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from existing index for hybrid retrieval...\n",
      "Hybrid retriever initialized with 1138 documents\n",
      "[Document(id='7f8e649e-124e-4c51-b2e2-e52cd9d1f54e', metadata={'id': 23}, page_content='. Here we explored the use of a retrieval-augmented generation (RAG) model which we tested on literature specific to a biomedical research area'), Document(id='8f71da88-b5e2-4512-9343-e68b5caf2130', metadata={'id': 33}, page_content='. The findings underscore the potential of RAG-enhanced language models to bridge the gap between complex biomedical literature and accessible public'), Document(id='19c8f245-4119-497b-8292-95a23b4d9632', metadata={'id': 33}, page_content='Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)This work presents a Biomedical Literature Question Answering (Q&A) system'), Document(metadata={'id': 33}, page_content='. Addressing the shortcomings of conventional health search engines and the lag in public access to biomedical research'), Document(id='2e115087-2960-4e98-917b-c9e4400e5ba0', metadata={'id': 32}, page_content='RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical Question AnsweringThe exponential growth of biomedical literature creates'), Document(metadata={'id': 33}, page_content='accessible public health knowledge'), Document(metadata={'id': 41}, page_content='. However, in the biomedical domain'), Document(id='b837ffca-1eba-4e89-9bef-84b62f054576', metadata={'id': 32}, page_content='. We present RAG-BioQA, a novel framework combining retrieval-augmented generation with domain-specific fine-tuning to produce evidence-based'), Document(metadata={'id': 49}, page_content='. We recruited fifteen professionals from various biomedical roles and industries to participate in sixty-minute semi-structured interviews'), Document(id='14e416e4-3a27-45c4-9cae-65b2306c308d', metadata={'id': 38}, page_content='Enhancing biomedical question answering with parameter-efficient finetuning and hierarchical retrieval augmented generation'), Document(metadata={'id': 49}, page_content='. We applied a qualitative analysis of individual interviews using an inductive-deductive thematic coding approach for emerging themes'), Document(id='37ab8a20-b7d8-490b-941b-574548effa65', metadata={'id': 34}, page_content='Reshaping Biomedical Scientific Literature in a RAG Pipeline for Question AnsweringBiomedical Question Answering (BQA) poses specific challenges due'), Document(id='7103ef4c-1064-4da3-bbee-8cf9f436e01d', metadata={'id': 56}, page_content=', up-to-date information via retrieval-augmented generation (RAG)'), Document(metadata={'id': 18}, page_content='attention in the biomedical domain'), Document(id='584d90b5-9cae-4144-8423-d0ba7c65aa0c', metadata={'id': 70}, page_content='LightRAG: Simple and Fast Retrieval-Augmented GenerationRetrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by'), Document(metadata={'id': 1}, page_content='. We present PaperQA, a RAG agent for answering questions over the scientific literature'), Document(id='feb59324-eb8e-4cb3-8001-14745daf4a3b', metadata={'id': 2}, page_content=', particularly when employing Retrieval Augmented Generation (RAG) techniques'), Document(metadata={'id': 19}, page_content='shift from general domain corpora to biomedical corpora'), Document(id='aaf16494-d177-4f93-a107-2be0533115d6', metadata={'id': 3}, page_content='Retrieval augmented generation for large language models in healthcare: A systematic reviewLarge Language Models (LLMs) have demonstrated promising'), Document(metadata={'id': 50}, page_content='. Biomedical literature access is essential for several types of users including biomedical researchers, clinicians, database curators'), Document(id='d28888be-ce73-4441-975e-70955142378c', metadata={'id': 38}, page_content=', which indicate that PEFT and RAG can significantly improve the performance in biomedical Question Answering (QA) tasks'), Document(metadata={'id': 50}, page_content='. Finally, the last section describes some predicted future trends for improving biomedical literature access'), Document(id='7e747538-6b8d-478f-925a-62003d39b4e2', metadata={'id': 66}, page_content='. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases'), Document(metadata={'id': 7}, page_content='SciBERT: A pretrained language model for scientific textObtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and'), Document(id='d4aa099b-9261-4364-bfd6-daf5770d1f43', metadata={'id': 56}, page_content='RadioRAG: Online Retrieval-Augmented Generation for Radiology Question AnsweringPurpose To evaluate diagnostic accuracy of various large language'), Document(metadata={'id': 71}, page_content='. This is evidence that generative models are good at aggregating and combining evidence from multiple passages.'), Document(id='78284b1c-b1a8-4ae5-86b5-e102d2e5d0b0', metadata={'id': 2}, page_content='Improving Retrieval for RAG based Question Answering Models on Financial DocumentsThe effectiveness of Large Language Models (LLMs) in generating'), Document(id='99c3be82-3cda-43a3-95b4-cea4cdbf9da7', metadata={'id': 33}, page_content='(Q&A) system based on a Retrieval-Augmented Generation (RAG) architecture'), Document(metadata={'id': 3}, page_content='. It is important to account for ethical challenges that are inherent when AI systems are implemented in the clinical setting'), Document(id='112f9b49-943e-4aee-9409-b170d8950740', metadata={'id': 53}, page_content='. Retrieval Augmented Generation (RAG) enables customization by integrating specialized knowledge'), Document(metadata={'id': 24}, page_content='. Our results demonstrate the potential for large language models to be effective tools in the clinical decision-making process'), Document(metadata={'id': 19}, page_content='Biobert: a pre-trained biomedical language representation model for biomedical text miningBiomedical text mining is becoming increasingly important as'), Document(id='00ed105f-ad27-4e95-a0c1-12cecef604ff', metadata={'id': 57}, page_content=', a Retrieval-Augmented Generation (RAG)-based AI assistant designed to deliver automated and clinically grounded responses to frequently asked'), Document(metadata={'id': 18}, page_content='. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for'), Document(id='81ae2ee3-7db2-493a-a439-6cd06666078a', metadata={'id': 41}, page_content='Incorporating entity-level knowledge in pretrained language model for biomedical dense retrievalIn recent years'), Document(metadata={'id': 8}, page_content='are tools used by the academic community for research and research evaluation that aggregate scientific literature output and measure impact by'), Document(id='49fe008b-a88e-416c-98f7-60e212e52dcb', metadata={'id': 1}, page_content='PaperQA: Retrieval-Augmented Generative Agent for Scientific ResearchLarge Language Models (LLMs) generalize well across language tasks'), Document(metadata={'id': 51}, page_content='. Retrieval-augmented generation (RAG) is an enterprise architecture that allows the embedding of customized data into LLMs')]\n",
      "✓ Retrieved 23 unique papers (from 50 chunks)\n",
      "\n",
      "Top 5 retrieved papers:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>Reshaping Biomedical Scientific Literature in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PaperQA: Retrieval-Augmented Generative Agent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>Accessing Biomedical Literature in the Current...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Improving accuracy of gpt-3/4 results on biome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Biobert: a pre-trained biomedical language rep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                              title\n",
       "33  34  Reshaping Biomedical Scientific Literature in ...\n",
       "0    1  PaperQA: Retrieval-Augmented Generative Agent ...\n",
       "49  50  Accessing Biomedical Literature in the Current...\n",
       "22  23  Improving accuracy of gpt-3/4 results on biome...\n",
       "18  19  Biobert: a pre-trained biomedical language rep..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Perform hybrid search\n",
    "rs = vector_store.hybrid_search(query, k=HYBRID_SEARCH_K)\n",
    "#rs = vector_store.semantic_search(query, k=HYBRID_SEARCH_K)\n",
    "\n",
    "print(rs)\n",
    "\n",
    "# Extract unique document IDs from results\n",
    "retrieved_docs = {item.metadata['id'] for item in rs}\n",
    "\n",
    "# Filter abstracts DataFrame to get full information for retrieved papers\n",
    "retrieved_abstracts = all_abstracts[all_abstracts['id'].isin(retrieved_docs)].copy()\n",
    "\n",
    "print(f\"✓ Retrieved {len(retrieved_abstracts)} unique papers (from {HYBRID_SEARCH_K} chunks)\")\n",
    "print(f\"\\nTop 5 retrieved papers:\")\n",
    "display(retrieved_abstracts[['id', 'title']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "retrieval-stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Statistics:\n",
      "  - Total papers in corpus: 78\n",
      "  - Papers retrieved: 23\n",
      "  - Retrieval rate: 29.5%\n",
      "\n",
      "Sample retrieved abstracts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>Reshaping Biomedical Scientific Literature in ...</td>\n",
       "      <td>Biomedical Question Answering (BQA) poses spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PaperQA: Retrieval-Augmented Generative Agent ...</td>\n",
       "      <td>Large Language Models (LLMs) generalize well a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>Accessing Biomedical Literature in the Current...</td>\n",
       "      <td>Biomedical and life sciences literature is uni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                              title  \\\n",
       "33  34  Reshaping Biomedical Scientific Literature in ...   \n",
       "0    1  PaperQA: Retrieval-Augmented Generative Agent ...   \n",
       "49  50  Accessing Biomedical Literature in the Current...   \n",
       "\n",
       "                                             abstract  \n",
       "33  Biomedical Question Answering (BQA) poses spec...  \n",
       "0   Large Language Models (LLMs) generalize well a...  \n",
       "49  Biomedical and life sciences literature is uni...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display retrieval statistics\n",
    "print(f\"Retrieval Statistics:\")\n",
    "print(f\"  - Total papers in corpus: {len(all_abstracts)}\")\n",
    "print(f\"  - Papers retrieved: {len(retrieved_abstracts)}\")\n",
    "print(f\"  - Retrieval rate: {len(retrieved_abstracts) / len(all_abstracts) * 100:.1f}%\")\n",
    "print(f\"\\nSample retrieved abstracts:\")\n",
    "display(retrieved_abstracts[['id', 'title', 'abstract']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Relevance Agent Setup\n",
    "\n",
    "Configure the relevance scoring agent that evaluates each paper using a debate-style approach:\n",
    "1. Generate arguments **for** including the paper\n",
    "2. Generate arguments **against** including the paper\n",
    "3. Extract supporting quotes from the abstract\n",
    "4. Assign a relevance probability score (1-100)\n",
    "\n",
    "This structured reasoning helps ensure high-quality relevance judgments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "define-relevance-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ AbstractRelevance model defined\n"
     ]
    }
   ],
   "source": [
    "# Define the structured output model for relevance scoring\n",
    "class AbstractRelevance(BaseModel):\n",
    "    \"\"\"Structured relevance assessment for a candidate paper.\"\"\"\n",
    "    id: int\n",
    "    arguments_for: str\n",
    "    arguments_for_quotes: list[str]\n",
    "    arguments_against: str\n",
    "    arguments_against_quotes: list[str]\n",
    "    probability_score: Annotated[\n",
    "        float, \n",
    "        Field(ge=1.0, le=100.0, description=\"A relevance score between 1 and 100.\")\n",
    "    ]\n",
    "\n",
    "print(\"✓ AbstractRelevance model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "create-agent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Relevance agent factory created\n"
     ]
    }
   ],
   "source": [
    "def create_relevance_agent():\n",
    "    \"\"\"Create an agent that scores paper relevance using debate-style reasoning.\"\"\"\n",
    "    \n",
    "    INSTRUCTIONS_DEBATE_RANKING = \"\"\" \n",
    "    You are a helpful research assistant who is helping with literature review of a research idea. \n",
    "    You will be given a query or research idea and a candidate reference abstract.\n",
    "    Your task is to score reference abstract based on their relevance to the query. Please make sure you read and understand these instructions carefully. \n",
    "    Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "    ## Instruction: \n",
    "    Use the following steps to rank the reference papers:\n",
    "\n",
    "    1. Generate arguments for including this reference abstract in the literature review.\n",
    "\n",
    "    2. Generate arguments against including this reference abstract in the literature review.\n",
    "\n",
    "    3. Extract relevant sentences from the candidate paper abstract to support each argument.\n",
    "\n",
    "    4. Then, provide a score between 1 and 100 (up to two decimal places) that is proportional to the probability \n",
    "    of a paper with the given query including the candidate reference paper in its literature review. \n",
    "\n",
    "    Important:\n",
    "    - Put the extracted sentences in quotes\n",
    "    - You can use the information in other candidate papers when generating the arguments for a candidate paper\n",
    "    - Generate arguments and probability for each paper separately\n",
    "    - Do not generate anything else apart from the probability and the arguments\n",
    "    - Follow this process even if a candidate paper happens to be identical or near-perfect match to the query abstract\n",
    "\n",
    "    Your Response: \"\"\"\n",
    "\n",
    "    relevance_agent = Agent(\n",
    "        name=\"RelevanceAgent\",\n",
    "        instructions=INSTRUCTIONS_DEBATE_RANKING,\n",
    "        model=RELEVANCE_MODEL,\n",
    "        output_type=AbstractRelevance\n",
    "    )\n",
    "    \n",
    "    return relevance_agent\n",
    "\n",
    "print(\"✓ Relevance agent factory created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "relevance-function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Relevance scoring function defined\n"
     ]
    }
   ],
   "source": [
    "async def relevance_summary(id: int, query: str, reference_paper: str) -> AbstractRelevance:\n",
    "    \"\"\"Score a single paper's relevance to the query using the relevance agent.\n",
    "    \n",
    "    Args:\n",
    "        id: Paper ID\n",
    "        query: Research query/abstract\n",
    "        reference_paper: Candidate paper's title and abstract\n",
    "    \n",
    "    Returns:\n",
    "        AbstractRelevance object with scoring and reasoning\n",
    "    \"\"\"\n",
    "    relevance_agent = create_relevance_agent()\n",
    "    \n",
    "    user_instructions = f\"\"\"\n",
    "For this query abstract with id={id}\n",
    "\n",
    "Given the query abstract: {query}\n",
    "\n",
    "Given the candidate reference paper abstract: {reference_paper}\n",
    "\n",
    "Your Reference Abstract Relevance:\n",
    "\"\"\"\n",
    "    \n",
    "    result = await Runner.run(relevance_agent, input=user_instructions)\n",
    "    return result.final_output\n",
    "\n",
    "print(\"✓ Relevance scoring function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scoring-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Parallel Relevance Scoring\n",
    "\n",
    "Score all retrieved papers in parallel using async execution for efficiency. Each paper is evaluated independently by the relevance agent.\n",
    "\n",
    "**Note**: Adjust `NUM_ABSTRACTS_TO_SCORE` in the configuration section to limit the number of papers scored (useful for testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "parallel-scoring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Parallel scoring function defined\n"
     ]
    }
   ],
   "source": [
    "async def gather_abstract_relevance(retrieved_abstracts: pd.DataFrame, num_to_score: int = None) -> List[AbstractRelevance]:\n",
    "    \"\"\"Score multiple abstracts in parallel.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_abstracts: DataFrame of retrieved papers\n",
    "        num_to_score: Number of abstracts to score (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        List of AbstractRelevance objects\n",
    "    \"\"\"\n",
    "    # Select subset if specified\n",
    "    if num_to_score is not None:\n",
    "        abstracts_to_score = retrieved_abstracts.head(num_to_score)\n",
    "        print(f\"Scoring {num_to_score} abstracts (configured limit)\")\n",
    "    else:\n",
    "        abstracts_to_score = retrieved_abstracts\n",
    "        print(f\"Scoring all {len(abstracts_to_score)} retrieved abstracts\")\n",
    "    \n",
    "    # Create async tasks for parallel execution\n",
    "    tasks = [\n",
    "        asyncio.create_task(\n",
    "            relevance_summary(\n",
    "                id=item['id'],\n",
    "                query=query,\n",
    "                reference_paper=item['title_abstract']\n",
    "            )\n",
    "        )\n",
    "        for index, item in abstracts_to_score[['id', 'title_abstract']].iterrows()\n",
    "    ]\n",
    "    \n",
    "    print(f\"Executing {len(tasks)} relevance scoring tasks in parallel...\")\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Parallel scoring function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "execute-scoring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring 10 abstracts (configured limit)\n",
      "Executing 10 relevance scoring tasks in parallel...\n",
      "\n",
      "✓ Completed scoring 10 abstracts\n",
      "CPU times: user 85.5 ms, sys: 25.1 ms, total: 111 ms\n",
      "Wall time: 29.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Execute relevance scoring (handles both Jupyter notebook and async contexts)\n",
    "try:\n",
    "    # Try to get existing event loop (in Jupyter)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    if loop.is_running():\n",
    "        # If loop is already running (Jupyter), use nest_asyncio or create task\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        results = loop.run_until_complete(\n",
    "            gather_abstract_relevance(retrieved_abstracts, NUM_ABSTRACTS_TO_SCORE)\n",
    "        )\n",
    "    else:\n",
    "        results = loop.run_until_complete(\n",
    "            gather_abstract_relevance(retrieved_abstracts, NUM_ABSTRACTS_TO_SCORE)\n",
    "        )\n",
    "except RuntimeError:\n",
    "    # If no event loop exists, create one\n",
    "    results = asyncio.run(\n",
    "        gather_abstract_relevance(retrieved_abstracts, NUM_ABSTRACTS_TO_SCORE)\n",
    "    )\n",
    "\n",
    "print(f\"\\n✓ Completed scoring {len(results)} abstracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "scoring-stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Scoring Statistics:\n",
      "  - Papers scored: 10\n",
      "  - Mean score: 74.08\n",
      "  - Std dev: 11.55\n",
      "  - Min score: 45.00\n",
      "  - Max score: 85.00\n",
      "  - Median score: 75.00\n",
      "\n",
      "Sample relevance assessments:\n",
      "\n",
      "  Paper ID 34 (Score: 80.00):\n",
      "    For: The candidate reference paper discusses the use of Retrieval-Augmented Generation (RAG) for biomedic...\n",
      "    Against: While the candidate paper discusses RAG and its applications in biomedical question answering, it do...\n",
      "\n",
      "  Paper ID 1 (Score: 85.00):\n",
      "    For: This reference abstract discusses a Retrieval-Augmented Generation (RAG) agent specifically tailored...\n",
      "    Against: The focus of the candidate paper on large-scale systematic processing of scientific knowledge may di...\n",
      "\n",
      "  Paper ID 50 (Score: 65.00):\n",
      "    For: The candidate reference paper discusses the challenges and state-of-the-art systems related to acces...\n",
      "    Against: The reference abstract primarily focuses on existing search tools and literature access strategies w...\n"
     ]
    }
   ],
   "source": [
    "# Display scoring statistics\n",
    "scores = [abs.probability_score for abs in results]\n",
    "\n",
    "print(\"Relevance Scoring Statistics:\")\n",
    "print(f\"  - Papers scored: {len(scores)}\")\n",
    "print(f\"  - Mean score: {np.mean(scores):.2f}\")\n",
    "print(f\"  - Std dev: {np.std(scores):.2f}\")\n",
    "print(f\"  - Min score: {np.min(scores):.2f}\")\n",
    "print(f\"  - Max score: {np.max(scores):.2f}\")\n",
    "print(f\"  - Median score: {np.median(scores):.2f}\")\n",
    "\n",
    "# Show sample of results\n",
    "print(f\"\\nSample relevance assessments:\")\n",
    "for i, result in enumerate(results[:3]):\n",
    "    print(f\"\\n  Paper ID {result.id} (Score: {result.probability_score:.2f}):\")\n",
    "    print(f\"    For: {result.arguments_for[:100]}...\")\n",
    "    print(f\"    Against: {result.arguments_against[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selection-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Top-K Selection\n",
    "\n",
    "Select the top-k most relevant papers based on their probability scores. These papers will be used to generate the \"Related Work\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "topk-function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Selected top 10 papers by relevance score\n"
     ]
    }
   ],
   "source": [
    "def get_top_k_abstracts(results: List[AbstractRelevance], k: int = 10) -> List[tuple]:\n",
    "    \"\"\"Select top-k papers by relevance score.\n",
    "    \n",
    "    Args:\n",
    "        results: List of AbstractRelevance objects\n",
    "        k: Number of top papers to select\n",
    "    \n",
    "    Returns:\n",
    "        List of (id, score) tuples sorted by score descending\n",
    "    \"\"\"\n",
    "    scores = [(abs.id, abs.probability_score) for abs in results]\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_scores[:k]\n",
    "\n",
    "# Get top-k papers\n",
    "top_k_scores = get_top_k_abstracts(results, k=TOP_K_PAPERS)\n",
    "\n",
    "print(f\"✓ Selected top {TOP_K_PAPERS} papers by relevance score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "display-topk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Papers Selected for Related Work:\n",
      "================================================================================\n",
      "\n",
      "[1] Score: 85.00\n",
      "Title: PaperQA: Retrieval-Augmented Generative Agent for Scientific Research\n",
      "Abstract: Large Language Models (LLMs) generalize well across language tasks, but suffer from hallucinations and uninterpretability, making it difficult to assess their accuracy without ground-truth. Retrieval-...\n",
      "\n",
      "[23] Score: 85.00\n",
      "Title: Improving accuracy of gpt-3/4 results on biomedical data using a retrieval-augmented language model.\n",
      "Abstract: Large language models (LLMs) have made a significant impact on the fields of general artificial intelligence. General purpose LLMs exhibit strong logic and reasoning skills and general world knowledge...\n",
      "\n",
      "[32] Score: 85.00\n",
      "Title: RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical Question Answering\n",
      "Abstract: The exponential growth of biomedical literature creates significant challenges for accessing precise medical information. Current biomedical question-answering systems primarily focus on short-form an...\n",
      "\n",
      "[34] Score: 80.00\n",
      "Title: Reshaping Biomedical Scientific Literature in a RAG Pipeline for Question Answering\n",
      "Abstract: Biomedical Question Answering (BQA) poses specific challenges due to the specialized vocabulary and complex semantic structures of biomedical literature. Large Language Models (LLMs) have shown great ...\n",
      "\n",
      "[19] Score: 75.00\n",
      "Title: Biobert: a pre-trained biomedical language representation model for biomedical text mining\n",
      "Abstract: Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information f...\n",
      "\n",
      "[51] Score: 75.00\n",
      "Title: Development of a liver disease-specific large language model chat interface using retrieval-augmented generation\n",
      "Abstract: Background and aims: Large language models (LLMs) have significant capabilities in clinical information processing tasks. Commercially available LLMs, however, are not optimized for clinical uses and ...\n",
      "\n",
      "[70] Score: 75.00\n",
      "Title: LightRAG: Simple and Fast Retrieval-Augmented Generation\n",
      "Abstract: Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user...\n",
      "\n",
      "[41] Score: 70.75\n",
      "Title: Incorporating entity-level knowledge in pretrained language model for biomedical dense retrieval\n",
      "Abstract: In recent years, pre-trained language models (PLMs) have dominated natural language processing (NLP) and achieved outstanding performance in various NLP tasks, including dense retrieval based on PLMs....\n",
      "\n",
      "[50] Score: 65.00\n",
      "Title: Accessing Biomedical Literature in the Current Information Landscape\n",
      "Abstract: Biomedical and life sciences literature is unique because of its exponentially increasing volume and interdisciplinary nature. Biomedical literature access is essential for several types of users incl...\n",
      "\n",
      "[8] Score: 45.00\n",
      "Title: scite: A smart citation index that displays the  context of citations and classifies their  intent using deep learning\n",
      "Abstract: Citation indices are tools used by the academic community for research and research evaluation that aggregate scientific literature output and measure impact by collating citation counts. Citation ind...\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>85.00</td>\n",
       "      <td>PaperQA: Retrieval-Augmented Generative Agent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>85.00</td>\n",
       "      <td>Improving accuracy of gpt-3/4 results on biome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>85.00</td>\n",
       "      <td>RAG-BioQA Retrieval-Augmented Generation for L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>80.00</td>\n",
       "      <td>Reshaping Biomedical Scientific Literature in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>75.00</td>\n",
       "      <td>Biobert: a pre-trained biomedical language rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>51</td>\n",
       "      <td>75.00</td>\n",
       "      <td>Development of a liver disease-specific large ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>70</td>\n",
       "      <td>75.00</td>\n",
       "      <td>LightRAG: Simple and Fast Retrieval-Augmented ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>70.75</td>\n",
       "      <td>Incorporating entity-level knowledge in pretra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>65.00</td>\n",
       "      <td>Accessing Biomedical Literature in the Current...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>45.00</td>\n",
       "      <td>scite: A smart citation index that displays th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  relevance_score                                              title\n",
       "0    1            85.00  PaperQA: Retrieval-Augmented Generative Agent ...\n",
       "22  23            85.00  Improving accuracy of gpt-3/4 results on biome...\n",
       "31  32            85.00  RAG-BioQA Retrieval-Augmented Generation for L...\n",
       "33  34            80.00  Reshaping Biomedical Scientific Literature in ...\n",
       "18  19            75.00  Biobert: a pre-trained biomedical language rep...\n",
       "50  51            75.00  Development of a liver disease-specific large ...\n",
       "69  70            75.00  LightRAG: Simple and Fast Retrieval-Augmented ...\n",
       "40  41            70.75  Incorporating entity-level knowledge in pretra...\n",
       "49  50            65.00  Accessing Biomedical Literature in the Current...\n",
       "7    8            45.00  scite: A smart citation index that displays th..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract top-k paper IDs and get full information\n",
    "top_k_id = [id for id, score in top_k_scores]\n",
    "top_k_abstracts = retrieved_abstracts[retrieved_abstracts['id'].isin(top_k_id)].copy()\n",
    "\n",
    "# Add scores to DataFrame for display\n",
    "score_dict = {id: score for id, score in top_k_scores}\n",
    "top_k_abstracts['relevance_score'] = top_k_abstracts['id'].map(score_dict)\n",
    "top_k_abstracts = top_k_abstracts.sort_values('relevance_score', ascending=False)\n",
    "\n",
    "print(f\"Top {TOP_K_PAPERS} Papers Selected for Related Work:\")\n",
    "print(\"=\" * 80)\n",
    "for idx, row in top_k_abstracts.iterrows():\n",
    "    print(f\"\\n[{row['id']}] Score: {row['relevance_score']:.2f}\")\n",
    "    print(f\"Title: {row['title']}\")\n",
    "    print(f\"Abstract: {row['abstract'][:200]}...\")\n",
    "\n",
    "# Display as DataFrame\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "display(top_k_abstracts[['id', 'relevance_score', 'title']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generation-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Related Work Generation\n",
    "\n",
    "Generate a cohesive \"Related Work\" section using the top-k papers. The generation agent:\n",
    "- Creates a coherent narrative connecting the papers\n",
    "- Performs critical analysis comparing strengths and weaknesses\n",
    "- Motivates the proposed approach in context of prior work\n",
    "- Cites papers using [id] format\n",
    "- Avoids copying abstracts verbatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "generation-prompt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generation instructions defined\n"
     ]
    }
   ],
   "source": [
    "# Define instructions for related work generation\n",
    "num_sentences = 10\n",
    "num_words = 450\n",
    "\n",
    "INSTRUCTIONS_RELATED_WORK = f\"\"\" \n",
    "You are an expert research assistant who is helping with literature review for a research idea or abstract. \n",
    "You will be provided with an abstract or research idea and a list of reference abstracts. \n",
    "Your task is to write the related work section of the document using only the provided reference abstracts. \n",
    "Please write the related work section creating a cohesive storyline by doing a critical analysis of prior work \n",
    "in the reference abstracts comparing the strengths and weaknesses while also motivating the proposed approach. \n",
    "You should cite the reference abstracts as [id] whenever you are referring it in the related work. \n",
    "Do not write it as Reference #. Do not cite abstract or research Idea. \n",
    "Do not include any extra notes or newline characters at the end. \n",
    "Do not copy the abstracts of reference papers directly but compare and contrast to the main work concisely. \n",
    "Do not provide the output in bullet points or markdown. \n",
    "Do not provide references at the end. \n",
    "Please generate {num_sentences} sentences in {num_words} words\n",
    "Please cite all the provided reference papers if needed.\n",
    "\"\"\"\n",
    "\n",
    "print(\"✓ Generation instructions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "build-generation-input",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Built generation input (16798 characters)\n"
     ]
    }
   ],
   "source": [
    "# Build input for related work generation\n",
    "input_related_work = f\"Given the Research Idea or abstract: {query}\"\n",
    "input_related_work += \"\\n\\n## Given references abstracts list below:\"\n",
    "\n",
    "for index, item in top_k_abstracts[['id', 'title_abstract']].iterrows():\n",
    "    input_related_work += f\"\\n\\n[{item['id']}]: {item['title_abstract']}\"\n",
    "\n",
    "input_related_work += \"\\n\\nWrite the related work section summarizing in a cohesive story prior works relevant to the research idea.\"\n",
    "input_related_work += \"\\n\\n## Related Work:\"\n",
    "\n",
    "print(f\"✓ Built generation input ({len(input_related_work)} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "generate-related-work",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Related work section generated\n",
      "  Length: 3289 characters\n",
      "  Words: ~419 words\n",
      "CPU times: user 12.2 ms, sys: 4.56 ms, total: 16.7 ms\n",
      "Wall time: 9.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Generate related work section\n",
    "response = openai_client.responses.create(\n",
    "    model=GENERATION_MODEL,\n",
    "    instructions=INSTRUCTIONS_RELATED_WORK,\n",
    "    input=input_related_work\n",
    ")\n",
    "\n",
    "generated_related_work = response.output_text\n",
    "\n",
    "print(\"✓ Related work section generated\")\n",
    "print(f\"  Length: {len(generated_related_work)} characters\")\n",
    "print(f\"  Words: ~{len(generated_related_work.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Results & Evaluation\n",
    "\n",
    "Display the final generated \"Related Work\" section with formatting and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "display-results",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Generated Related Work Section"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Retrieval-augmented generation (RAG) systems have emerged as pivotal tools to tackle the complexities of biomedical question answering, albeit with varying effectiveness across implementations. For instance, PaperQA demonstrates a promising application by integrating information retrieval from full-text articles, thus surpassing the performance of standard large language models (LLMs) on specific science QA benchmarks, notably through the introduction of the LitQA benchmark, which emphasizes a human-like research methodology for synthesizing information from diverse sources [1]. However, while PaperQA excels in retrieval capabilities, it still grapples with the issue of hallucinations that plague many LLM applications. Addressing this, another study emphasizes the adaptation of general-purpose LLMs through specialized corpora to minimize inaccuracies when querying around biomedical contexts, highlighting that a RAG model significantly outperformed other models in accuracy and relevance on specific questions tied to diffuse large B-cell lymphoma [23]. \n",
       "\n",
       "In addition, RAG-BioQA pushes forward the agenda within long-form biomedical answers, combining retrieval-augmented generation with domain-specific fine-tuning, thereby showcasing that comprehensive and evidence-based information is necessary for clinical decision-making—a critical gap in existing short-answer frameworks [32]. This resonates with findings that validate the necessity of contextual understanding in RAG frameworks, noting that the quality of generated answers can be positively influenced by the structuring of contextual information, as discussed in another investigation of context-grounded RAG pipelines for biomedical literature [34]. \n",
       "\n",
       "Overall, while existing RAG systems provide substantive groundwork with model enhancements and specialized training like those realized through BioBERT, which greatly improves on biomedical text mining tasks [19], they often fall short in optimizing for clinical and contextual nuances. Additional challenges have been raised regarding deficiencies in comprehensiveness and safety within the outputs of specialized LLMs such as LiVersa, despite their high accuracy rates [51]. To tackle the shortcomings of existing RAG approaches, a newer methodology, such as LightRAG, proposes innovations including dual-level retrieval frameworks that leverage graph structures, significantly enhancing context awareness and retrieval efficiency, thus addressing key limitations in traditional models [70]. \n",
       "\n",
       "Importantly, as the volume of biomedical literature continually grows, the evolving landscape of literature access necessitates enhanced models that can effectively manage query formulation and improve result interpretation [50]. Recent studies indicate that understanding the semantic complexities in biomedical literature can further enhance retrieval systems through the integration of entity-level knowledge, thus achieving superior performance in biomedical dense retrieval scenarios [41]. Collectively, these prior works underline the ongoing need for a high-performing RAG strategy tailored to the biomedical domain, reinforcing the aim of this proposed research to provide clear and accessible evidence for professionals engaging with scientific literature."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the generated related work section\n",
    "display(Markdown(\"## Generated Related Work Section\"))\n",
    "display(Markdown(\"---\"))\n",
    "display(Markdown(generated_related_work))\n",
    "display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "citations-used",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citations Used in Generated Text:\n",
      "  - Total citations: 9\n",
      "  - Unique papers cited: 9\n",
      "  - Papers provided: 10\n",
      "  - Citation IDs: [1, 19, 23, 32, 34, 41, 50, 51, 70]\n",
      "\n",
      "Cited Papers:\n",
      "  [1] PaperQA: Retrieval-Augmented Generative Agent for Scientific Research\n",
      "  [19] Biobert: a pre-trained biomedical language representation model for biomedical text mining\n",
      "  [23] Improving accuracy of gpt-3/4 results on biomedical data using a retrieval-augmented language model.\n",
      "  [32] RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical Question Answering\n",
      "  [34] Reshaping Biomedical Scientific Literature in a RAG Pipeline for Question Answering\n",
      "  [41] Incorporating entity-level knowledge in pretrained language model for biomedical dense retrieval\n",
      "  [50] Accessing Biomedical Literature in the Current Information Landscape\n",
      "  [51] Development of a liver disease-specific large language model chat interface using retrieval-augmented generation\n",
      "  [70] LightRAG: Simple and Fast Retrieval-Augmented Generation\n"
     ]
    }
   ],
   "source": [
    "# Extract citations used in the generated text\n",
    "import re\n",
    "\n",
    "citations = re.findall(r'\\[(\\d+)\\]', generated_related_work)\n",
    "unique_citations = sorted(set(int(c) for c in citations))\n",
    "\n",
    "print(f\"Citations Used in Generated Text:\")\n",
    "print(f\"  - Total citations: {len(citations)}\")\n",
    "print(f\"  - Unique papers cited: {len(unique_citations)}\")\n",
    "print(f\"  - Papers provided: {len(top_k_abstracts)}\")\n",
    "print(f\"  - Citation IDs: {unique_citations}\")\n",
    "\n",
    "# Show which papers were cited\n",
    "print(f\"\\nCited Papers:\")\n",
    "for paper_id in unique_citations:\n",
    "    paper = top_k_abstracts[top_k_abstracts['id'] == paper_id]\n",
    "    if not paper.empty:\n",
    "        print(f\"  [{paper_id}] {paper.iloc[0]['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "pipeline-summary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Pipeline Execution Summary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Configuration:**\n",
       "- Corpus size: 78 papers\n",
       "- Hybrid retrieval: Top 50 papers\n",
       "- Papers retrieved: 23 unique papers\n",
       "- Papers scored: 10 papers\n",
       "- Top-K selection: 10 papers\n",
       "- Papers cited in output: 9 papers\n",
       "\n",
       "**Models Used:**\n",
       "- Relevance scoring: gpt-4o-mini\n",
       "- Related work generation: gpt-4o-mini\n",
       "\n",
       "**Output:**\n",
       "- Related work length: 3289 characters (~419 words)\n",
       "- Citations included: 9 total, 9 unique\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pipeline execution summary\n",
    "display(Markdown(\"## Pipeline Execution Summary\"))\n",
    "\n",
    "summary = f\"\"\"\n",
    "**Configuration:**\n",
    "- Corpus size: {len(all_abstracts)} papers\n",
    "- Hybrid retrieval: Top {HYBRID_SEARCH_K} papers\n",
    "- Papers retrieved: {len(retrieved_abstracts)} unique papers\n",
    "- Papers scored: {len(results)} papers\n",
    "- Top-K selection: {TOP_K_PAPERS} papers\n",
    "- Papers cited in output: {len(unique_citations)} papers\n",
    "\n",
    "**Models Used:**\n",
    "- Relevance scoring: {RELEVANCE_MODEL}\n",
    "- Related work generation: {GENERATION_MODEL}\n",
    "\n",
    "**Output:**\n",
    "- Related work length: {len(generated_related_work)} characters (~{len(generated_related_work.split())} words)\n",
    "- Citations included: {len(citations)} total, {len(unique_citations)} unique\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "save-output",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Output saved to generated_related_work.txt\n"
     ]
    }
   ],
   "source": [
    "# Optional: Save the generated related work to a file\n",
    "SAVE_OUTPUT = True  # Set to True to save\n",
    "\n",
    "if SAVE_OUTPUT:\n",
    "    output_file = \"generated_related_work.txt\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"RESEARCH QUERY:\\n\")\n",
    "        f.write(query)\n",
    "        f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "        f.write(\"RELATED WORK:\\n\")\n",
    "        f.write(generated_related_work)\n",
    "        f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "        f.write(\"REFERENCES:\\n\")\n",
    "        for paper_id in unique_citations:\n",
    "            paper = top_k_abstracts[top_k_abstracts['id'] == paper_id]\n",
    "            if not paper.empty:\n",
    "                f.write(f\"[{paper_id}] {paper.iloc[0]['title']}\\n\")\n",
    "    \n",
    "    print(f\"✓ Output saved to {output_file}\")\n",
    "else:\n",
    "    print(\"Output not saved (set SAVE_OUTPUT=True to save)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated an end-to-end agentic RAG pipeline for automated literature review generation. Key features:\n",
    "\n",
    "1. **Hybrid Retrieval**: Combines semantic and keyword search for comprehensive coverage\n",
    "2. **Agentic Scoring**: Uses structured reasoning (debate-style) for reliable relevance assessment\n",
    "3. **Parallel Processing**: Efficiently scores multiple papers concurrently\n",
    "4. **Coherent Synthesis**: Generates well-structured literature reviews with proper citations\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with different retrieval parameters (`HYBRID_SEARCH_K`)\n",
    "- Adjust the number of papers to score (`NUM_ABSTRACTS_TO_SCORE`)\n",
    "- Try different top-k values (`TOP_K_PAPERS`)\n",
    "- Evaluate different LLM models for scoring and generation\n",
    "- Expand the corpus with more biomedical abstracts\n",
    "- Add evaluation metrics for generated related work quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-lit-llm (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
