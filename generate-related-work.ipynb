{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Automated Literature Review Generation using Agentic RAG\n",
    "\n",
    "This notebook demonstrates an end-to-end pipeline for automatically generating \"Related Work\" sections for scientific papers using:\n",
    "- **Hybrid Retrieval**: Combining semantic search (vector embeddings) and keyword search (BM25)\n",
    "- **Agentic Relevance Scoring**: Using LLM agents to evaluate paper relevance with structured reasoning\n",
    "- **Automated Synthesis**: Generating coherent literature reviews with proper citations\n",
    "\n",
    "## Pipeline Overview\n",
    "1. Load biomedical abstracts corpus\n",
    "2. Create vector store with hybrid retrieval capabilities\n",
    "3. Define research query/abstract\n",
    "4. Retrieve candidate papers using hybrid search\n",
    "5. Score papers using debate-style relevance agent\n",
    "6. Select top-k most relevant papers\n",
    "7. Generate cohesive \"Related Work\" section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Configuration\n",
    "\n",
    "Import dependencies and configure pipeline parameters. Adjust these settings to customize the pipeline behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "autoreload",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-reload modules for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import asyncio\n",
    "from typing import List, Dict, Any\n",
    "from pprint import pprint\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# OpenAI and agents\n",
    "import openai\n",
    "from agents import Agent, Runner\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated\n",
    "\n",
    "# Environment and display\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, HTML\n",
    "\n",
    "# Local modules\n",
    "from vectorstore import VectorStoreAbstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configuration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n",
      "  - Retrieval: Top 50 papers using hybrid search\n",
      "  - Scoring: 3 abstracts will be scored\n",
      "  - Selection: Top 3 papers for related work\n",
      "  - Models: gpt-4o-mini (scoring), gpt-4o-mini (generation)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Vector Store Configuration\n",
    "CHROMA_PERSIST_DIRECTORY = \"./corpus-data/chroma_db\"\n",
    "RECREATE_INDEX = False  # Set to True to rebuild the index from scratch\n",
    "\n",
    "# Retrieval Configuration\n",
    "HYBRID_SEARCH_K = 50  # Number of papers to retrieve using hybrid search\n",
    "\n",
    "# Relevance Scoring Configuration\n",
    "NUM_ABSTRACTS_TO_SCORE = 3  # Set to None to score all retrieved abstracts, or set a number for testing (e.g., 5, 10, 20)\n",
    "RELEVANCE_MODEL = \"gpt-4o-mini\"  # Model for relevance scoring agent\n",
    "\n",
    "# Top-K Selection Configuration\n",
    "TOP_K_PAPERS = 3  # Number of top-ranked papers to include in related work\n",
    "\n",
    "# Related Work Generation Configuration\n",
    "GENERATION_MODEL = \"gpt-4o-mini\"  # Model for generating related work section\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"  - Retrieval: Top {HYBRID_SEARCH_K} papers using hybrid search\")\n",
    "print(f\"  - Scoring: {'All' if NUM_ABSTRACTS_TO_SCORE is None else NUM_ABSTRACTS_TO_SCORE} abstracts will be scored\")\n",
    "print(f\"  - Selection: Top {TOP_K_PAPERS} papers for related work\")\n",
    "print(f\"  - Models: {RELEVANCE_MODEL} (scoring), {GENERATION_MODEL} (generation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "env-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment loaded\n",
      "✓ OpenAI client initialized\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables and initialize OpenAI client\n",
    "load_dotenv(override=True)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_client = openai.OpenAI()\n",
    "\n",
    "print(\"✓ Environment loaded\")\n",
    "print(\"✓ OpenAI client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Loading & Preparation\n",
    "\n",
    "Load the biomedical abstracts corpus and prepare it for indexing. Each abstract contains:\n",
    "- **id**: Unique identifier\n",
    "- **title**: Paper title\n",
    "- **abstract**: Paper abstract\n",
    "- **title_abstract**: Concatenated title and abstract for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78 abstracts from corpus\n",
      "\n",
      "Dataset columns: ['id', 'title', 'abstract']\n",
      "\n",
      "First few abstracts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>Reshaping Biomedical Scientific Literature in ...</td>\n",
       "      <td>Biomedical Question Answering (BQA) poses spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PaperQA: Retrieval-Augmented Generative Agent ...</td>\n",
       "      <td>Large Language Models (LLMs) generalize well a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>Attention is all you need, A</td>\n",
       "      <td>The dominant sequence transduction models are ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                              title  \\\n",
       "33  34  Reshaping Biomedical Scientific Literature in ...   \n",
       "0    1  PaperQA: Retrieval-Augmented Generative Agent ...   \n",
       "34  35                       Attention is all you need, A   \n",
       "\n",
       "                                             abstract  \n",
       "33  Biomedical Question Answering (BQA) poses spec...  \n",
       "0   Large Language Models (LLMs) generalize well a...  \n",
       "34  The dominant sequence transduction models are ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load abstracts from CSV and shuffle\n",
    "all_abstracts = pd.read_csv('./abstracts_rag.csv').sample(frac=1, random_state=42)\n",
    "\n",
    "# Set to True to delete existing database\n",
    "RECREATE_INDEX = False\n",
    "\n",
    "print(f\"Loaded {len(all_abstracts)} abstracts from corpus\")\n",
    "print(f\"\\nDataset columns: {list(all_abstracts.columns)}\")\n",
    "print(f\"\\nFirst few abstracts:\")\n",
    "all_abstracts.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "prepare-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prepared 78 abstracts for indexing\n",
      "\n",
      "Sample abstract structure:\n",
      "  - ID: 34\n",
      "  - Text length: 1325 characters\n"
     ]
    }
   ],
   "source": [
    "# Concatenate title and abstract for better retrieval\n",
    "all_abstracts['title_abstract'] = all_abstracts['title'] + all_abstracts['abstract']\n",
    "\n",
    "# Convert to list of dictionaries for vector store\n",
    "samples_abstracts = [\n",
    "    v for k, v in all_abstracts[['title_abstract', 'id']].reset_index(drop=True).T.to_dict().items()\n",
    "]\n",
    "\n",
    "print(f\"✓ Prepared {len(samples_abstracts)} abstracts for indexing\")\n",
    "print(f\"\\nSample abstract structure:\")\n",
    "print(f\"  - ID: {samples_abstracts[0]['id']}\")\n",
    "print(f\"  - Text length: {len(samples_abstracts[0]['title_abstract'])} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vectorstore-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Vector Store Initialization\n",
    "\n",
    "Initialize ChromaDB vector store with hybrid retrieval capabilities:\n",
    "- **Semantic Search**: Uses HuggingFace embeddings (all-MiniLM-L6-v2)\n",
    "- **Keyword Search**: Uses BM25 algorithm\n",
    "- **Chunking**: Splits abstracts into 150-character chunks with 20-character overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "init-vectorstore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreating existing index at ./corpus-data/chroma_db\n",
      "✓ Using existing index at ./corpus-data/chroma_db\n",
      "  Index contains 1138 document chunks\n"
     ]
    }
   ],
   "source": [
    "# Initialize vector store\n",
    "vector_store = VectorStoreAbstract(\n",
    "    abstracts=samples_abstracts,\n",
    "    persist_directory=CHROMA_PERSIST_DIRECTORY,\n",
    "    recreate_index=RECREATE_INDEX\n",
    ")\n",
    "\n",
    "# Display index status\n",
    "if vector_store.index_exists:\n",
    "    doc_count = vector_store.get_document_count()\n",
    "    print(f\"✓ Using existing index at {CHROMA_PERSIST_DIRECTORY}\")\n",
    "    print(f\"  Index contains {doc_count} document chunks\")\n",
    "else:\n",
    "    print(f\"✓ Created new index at {CHROMA_PERSIST_DIRECTORY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "chunk-documents",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking documents: 100%|██████████| 78/78 [00:00<00:00, 6998.73article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 1138 document chunks\n",
      "CPU times: user 9.56 ms, sys: 15.9 ms, total: 25.5 ms\n",
      "Wall time: 26.6 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Chunk documents if needed (only when creating new index or recreating)\n",
    "if vector_store.should_process_documents():\n",
    "    print(\"Chunking documents...\")\n",
    "    documents = vector_store.chunking()\n",
    "    print(f\"✓ Created {len(documents)} document chunks\")\n",
    "else:\n",
    "    print(\"✓ Skipping document chunking (using existing index)\")\n",
    "    documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "index-documents",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing 1138 documents (this may take several minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings: 100%|██████████| 1138/1138 [00:02<00:00, 423.78doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Indexing completed!\n",
      "  Total chunks indexed: 2276\n",
      "CPU times: user 1.51 s, sys: 483 ms, total: 1.99 s\n",
      "Wall time: 2.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Index documents if needed\n",
    "if vector_store.should_process_documents():\n",
    "    print(f\"Indexing {len(documents)} documents (this may take several minutes)...\")\n",
    "    vector_store.index_document(documents)\n",
    "    print(\"✓ Indexing completed!\")\n",
    "    print(f\"  Total chunks indexed: {vector_store.get_document_count()}\")\n",
    "else:\n",
    "    print(\"✓ Skipping document indexing (using existing index)\")\n",
    "    print(f\"  Ready to perform searches!\")\n",
    "    print(f\"  Index contains {vector_store.get_document_count()} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Research Query Definition\n",
    "\n",
    "Define the research query or abstract for which we want to generate a literature review. This will be used to:\n",
    "1. Retrieve relevant papers from the corpus\n",
    "2. Score the relevance of each retrieved paper\n",
    "3. Generate the final \"Related Work\" section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "define-query",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Research Query/Abstract"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "_Retrieval-augmented generation (RAG) systems are emerging as effective tools for biomedical literature. \n",
       "However, their performance in this domain is not yet generalizable. \n",
       "We propose a new strategy for high-performing RAG applied to biomedical question answering. \n",
       "This approach would allow the wider public and public health professionals to access evidence from scientific literature in easy-to-understand language._"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query length: 419 characters\n"
     ]
    }
   ],
   "source": [
    "# Define the research query/abstract\n",
    "query = \"\"\"\n",
    "Retrieval-augmented generation (RAG) systems are emerging as effective tools for biomedical literature. \n",
    "However, their performance in this domain is not yet generalizable. \n",
    "We propose a new strategy for high-performing RAG applied to biomedical question answering. \n",
    "This approach would allow the wider public and public health professionals to access evidence from scientific literature in easy-to-understand language.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Display the query\n",
    "display(Markdown(\"### Research Query/Abstract\"))\n",
    "display(Markdown(f\"_{query}_\"))\n",
    "print(f\"\\nQuery length: {len(query)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retrieval-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Hybrid Retrieval\n",
    "\n",
    "Perform hybrid search combining:\n",
    "- **Semantic similarity**: Vector search using embeddings\n",
    "- **Keyword matching**: BM25 ranking\n",
    "\n",
    "The ensemble retriever combines both methods with equal weights (0.5, 0.5) to balance semantic understanding and keyword relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hybrid-search",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='5adf333d-e035-4df4-aa49-ec5f055c0eb7', metadata={'id': 23}, page_content='. Here we explored the use of a retrieval-augmented generation (RAG) model which we tested on literature specific to a biomedical research area'), Document(id='3c19f0ee-607a-40ec-92d4-84cd5985957e', metadata={'id': 33}, page_content='Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)This work presents a Biomedical Literature Question Answering (Q&A) system'), Document(id='ee134bfb-ea31-46d9-bd43-3b470b818f9a', metadata={'id': 32}, page_content='RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical Question AnsweringThe exponential growth of biomedical literature creates'), Document(id='87ad8796-a769-4f5c-b5af-5df5acd50865', metadata={'id': 32}, page_content='. We present RAG-BioQA, a novel framework combining retrieval-augmented generation with domain-specific fine-tuning to produce evidence-based'), Document(id='120bb809-5b8e-4466-8692-bacfa339414f', metadata={'id': 38}, page_content='Enhancing biomedical question answering with parameter-efficient finetuning and hierarchical retrieval augmented generation'), Document(id='a03742ec-7888-451a-9bff-7aa2e9e0b919', metadata={'id': 34}, page_content='Reshaping Biomedical Scientific Literature in a RAG Pipeline for Question AnsweringBiomedical Question Answering (BQA) poses specific challenges due'), Document(id='bace4be6-3ee1-4dfc-83b4-633871b48599', metadata={'id': 56}, page_content=', up-to-date information via retrieval-augmented generation (RAG)'), Document(id='e0045d8b-6cf6-46e5-9d06-c3d7c7bb23b6', metadata={'id': 70}, page_content='LightRAG: Simple and Fast Retrieval-Augmented GenerationRetrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by'), Document(id='e688a6d4-a10b-4e21-b21b-e6a62c7c5c16', metadata={'id': 2}, page_content=', particularly when employing Retrieval Augmented Generation (RAG) techniques'), Document(id='c9ed91e9-26b7-4d57-b609-798c943eb5c2', metadata={'id': 3}, page_content='Retrieval augmented generation for large language models in healthcare: A systematic reviewLarge Language Models (LLMs) have demonstrated promising'), Document(metadata={'id': 33}, page_content='. Addressing the shortcomings of conventional health search engines and the lag in public access to biomedical research'), Document(metadata={'id': 33}, page_content='accessible public health knowledge'), Document(metadata={'id': 41}, page_content='. However, in the biomedical domain'), Document(metadata={'id': 49}, page_content='. We recruited fifteen professionals from various biomedical roles and industries to participate in sixty-minute semi-structured interviews'), Document(metadata={'id': 49}, page_content='. We applied a qualitative analysis of individual interviews using an inductive-deductive thematic coding approach for emerging themes'), Document(metadata={'id': 33}, page_content='. The findings underscore the potential of RAG-enhanced language models to bridge the gap between complex biomedical literature and accessible public'), Document(metadata={'id': 18}, page_content='attention in the biomedical domain'), Document(metadata={'id': 1}, page_content='. We present PaperQA, a RAG agent for answering questions over the scientific literature'), Document(metadata={'id': 19}, page_content='shift from general domain corpora to biomedical corpora'), Document(metadata={'id': 50}, page_content='. Biomedical literature access is essential for several types of users including biomedical researchers, clinicians, database curators'), Document(metadata={'id': 50}, page_content='. Finally, the last section describes some predicted future trends for improving biomedical literature access'), Document(metadata={'id': 7}, page_content='SciBERT: A pretrained language model for scientific textObtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and'), Document(metadata={'id': 71}, page_content='. This is evidence that generative models are good at aggregating and combining evidence from multiple passages.'), Document(metadata={'id': 3}, page_content='. It is important to account for ethical challenges that are inherent when AI systems are implemented in the clinical setting'), Document(metadata={'id': 24}, page_content='. Our results demonstrate the potential for large language models to be effective tools in the clinical decision-making process'), Document(metadata={'id': 19}, page_content='Biobert: a pre-trained biomedical language representation model for biomedical text miningBiomedical text mining is becoming increasingly important as'), Document(metadata={'id': 18}, page_content='. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for'), Document(metadata={'id': 8}, page_content='are tools used by the academic community for research and research evaluation that aggregate scientific literature output and measure impact by'), Document(metadata={'id': 51}, page_content='. Retrieval-augmented generation (RAG) is an enterprise architecture that allows the embedding of customized data into LLMs')]\n",
      "✓ Retrieved 20 unique papers (from 50 chunks)\n",
      "\n",
      "Top 5 retrieved papers:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>Reshaping Biomedical Scientific Literature in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PaperQA: Retrieval-Augmented Generative Agent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>Accessing Biomedical Literature in the Current...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Improving accuracy of gpt-3/4 results on biome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Biobert: a pre-trained biomedical language rep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                              title\n",
       "33  34  Reshaping Biomedical Scientific Literature in ...\n",
       "0    1  PaperQA: Retrieval-Augmented Generative Agent ...\n",
       "49  50  Accessing Biomedical Literature in the Current...\n",
       "22  23  Improving accuracy of gpt-3/4 results on biome...\n",
       "18  19  Biobert: a pre-trained biomedical language rep..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Perform hybrid search\n",
    "rs = vector_store.hybrid_search(query, k=HYBRID_SEARCH_K)\n",
    "#rs = vector_store.semantic_search(query, k=HYBRID_SEARCH_K)\n",
    "\n",
    "print(rs)\n",
    "\n",
    "# Extract unique document IDs from results\n",
    "retrieved_docs = {item.metadata['id'] for item in rs}\n",
    "\n",
    "# Filter abstracts DataFrame to get full information for retrieved papers\n",
    "retrieved_abstracts = all_abstracts[all_abstracts['id'].isin(retrieved_docs)].copy()\n",
    "\n",
    "print(f\"✓ Retrieved {len(retrieved_abstracts)} unique papers (from {HYBRID_SEARCH_K} chunks)\")\n",
    "print(f\"\\nTop 5 retrieved papers:\")\n",
    "display(retrieved_abstracts[['id', 'title']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "retrieval-stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Statistics:\n",
      "  - Total papers in corpus: 78\n",
      "  - Papers retrieved: 20\n",
      "  - Retrieval rate: 25.6%\n",
      "\n",
      "Sample retrieved abstracts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>Reshaping Biomedical Scientific Literature in ...</td>\n",
       "      <td>Biomedical Question Answering (BQA) poses spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PaperQA: Retrieval-Augmented Generative Agent ...</td>\n",
       "      <td>Large Language Models (LLMs) generalize well a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>Accessing Biomedical Literature in the Current...</td>\n",
       "      <td>Biomedical and life sciences literature is uni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                              title  \\\n",
       "33  34  Reshaping Biomedical Scientific Literature in ...   \n",
       "0    1  PaperQA: Retrieval-Augmented Generative Agent ...   \n",
       "49  50  Accessing Biomedical Literature in the Current...   \n",
       "\n",
       "                                             abstract  \n",
       "33  Biomedical Question Answering (BQA) poses spec...  \n",
       "0   Large Language Models (LLMs) generalize well a...  \n",
       "49  Biomedical and life sciences literature is uni...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display retrieval statistics\n",
    "print(f\"Retrieval Statistics:\")\n",
    "print(f\"  - Total papers in corpus: {len(all_abstracts)}\")\n",
    "print(f\"  - Papers retrieved: {len(retrieved_abstracts)}\")\n",
    "print(f\"  - Retrieval rate: {len(retrieved_abstracts) / len(all_abstracts) * 100:.1f}%\")\n",
    "print(f\"\\nSample retrieved abstracts:\")\n",
    "display(retrieved_abstracts[['id', 'title', 'abstract']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Relevance Agent Setup\n",
    "\n",
    "Configure the relevance scoring agent that evaluates each paper using a debate-style approach:\n",
    "1. Generate arguments **for** including the paper\n",
    "2. Generate arguments **against** including the paper\n",
    "3. Extract supporting quotes from the abstract\n",
    "4. Assign a relevance probability score (1-100)\n",
    "\n",
    "This structured reasoning helps ensure high-quality relevance judgments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "define-relevance-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ AbstractRelevance model defined\n"
     ]
    }
   ],
   "source": [
    "# Define the structured output model for relevance scoring\n",
    "class AbstractRelevance(BaseModel):\n",
    "    \"\"\"Structured relevance assessment for a candidate paper.\"\"\"\n",
    "    id: int\n",
    "    arguments_for: str\n",
    "    arguments_for_quotes: list[str]\n",
    "    arguments_against: str\n",
    "    arguments_against_quotes: list[str]\n",
    "    probability_score: Annotated[\n",
    "        float, \n",
    "        Field(ge=1.0, le=100.0, description=\"A relevance score between 1 and 100.\")\n",
    "    ]\n",
    "\n",
    "print(\"✓ AbstractRelevance model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "create-agent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Relevance agent factory created\n"
     ]
    }
   ],
   "source": [
    "def create_relevance_agent():\n",
    "    \"\"\"Create an agent that scores paper relevance using debate-style reasoning.\"\"\"\n",
    "    \n",
    "    INSTRUCTIONS_DEBATE_RANKING = \"\"\" \n",
    "    You are a helpful research assistant who is helping with literature review of a research idea. \n",
    "    You will be given a query or research idea and a candidate reference abstract.\n",
    "    Your task is to score reference abstract based on their relevance to the query. Please make sure you read and understand these instructions carefully. \n",
    "    Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "    ## Instruction: \n",
    "    Use the following steps to rank the reference papers:\n",
    "\n",
    "    1. Generate arguments for including this reference abstract in the literature review.\n",
    "\n",
    "    2. Generate arguments against including this reference abstract in the literature review.\n",
    "\n",
    "    3. Extract relevant sentences from the candidate paper abstract to support each argument.\n",
    "\n",
    "    4. Then, provide a score between 1 and 100 (up to two decimal places) that is proportional to the probability \n",
    "    of a paper with the given query including the candidate reference paper in its literature review. \n",
    "\n",
    "    Important:\n",
    "    - Put the extracted sentences in quotes\n",
    "    - You can use the information in other candidate papers when generating the arguments for a candidate paper\n",
    "    - Generate arguments and probability for each paper separately\n",
    "    - Do not generate anything else apart from the probability and the arguments\n",
    "    - Follow this process even if a candidate paper happens to be identical or near-perfect match to the query abstract\n",
    "\n",
    "    Your Response: \"\"\"\n",
    "\n",
    "    relevance_agent = Agent(\n",
    "        name=\"RelevanceAgent\",\n",
    "        instructions=INSTRUCTIONS_DEBATE_RANKING,\n",
    "        model=RELEVANCE_MODEL,\n",
    "        output_type=AbstractRelevance\n",
    "    )\n",
    "    \n",
    "    return relevance_agent\n",
    "\n",
    "print(\"✓ Relevance agent factory created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "relevance-function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Relevance scoring function defined\n"
     ]
    }
   ],
   "source": [
    "async def relevance_summary(id: int, query: str, reference_paper: str) -> AbstractRelevance:\n",
    "    \"\"\"Score a single paper's relevance to the query using the relevance agent.\n",
    "    \n",
    "    Args:\n",
    "        id: Paper ID\n",
    "        query: Research query/abstract\n",
    "        reference_paper: Candidate paper's title and abstract\n",
    "    \n",
    "    Returns:\n",
    "        AbstractRelevance object with scoring and reasoning\n",
    "    \"\"\"\n",
    "    relevance_agent = create_relevance_agent()\n",
    "    \n",
    "    user_instructions = f\"\"\"\n",
    "For this query abstract with id={id}\n",
    "\n",
    "Given the query abstract: {query}\n",
    "\n",
    "Given the candidate reference paper abstract: {reference_paper}\n",
    "\n",
    "Your Reference Abstract Relevance:\n",
    "\"\"\"\n",
    "    \n",
    "    result = await Runner.run(relevance_agent, input=user_instructions)\n",
    "    return result.final_output\n",
    "\n",
    "print(\"✓ Relevance scoring function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scoring-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Parallel Relevance Scoring\n",
    "\n",
    "Score all retrieved papers in parallel using async execution for efficiency. Each paper is evaluated independently by the relevance agent.\n",
    "\n",
    "**Note**: Adjust `NUM_ABSTRACTS_TO_SCORE` in the configuration section to limit the number of papers scored (useful for testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "parallel-scoring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Parallel scoring function defined\n"
     ]
    }
   ],
   "source": [
    "async def gather_abstract_relevance(retrieved_abstracts: pd.DataFrame, num_to_score: int = None) -> List[AbstractRelevance]:\n",
    "    \"\"\"Score multiple abstracts in parallel.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_abstracts: DataFrame of retrieved papers\n",
    "        num_to_score: Number of abstracts to score (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        List of AbstractRelevance objects\n",
    "    \"\"\"\n",
    "    # Select subset if specified\n",
    "    if num_to_score is not None:\n",
    "        abstracts_to_score = retrieved_abstracts.head(num_to_score)\n",
    "        print(f\"Scoring {num_to_score} abstracts (configured limit)\")\n",
    "    else:\n",
    "        abstracts_to_score = retrieved_abstracts\n",
    "        print(f\"Scoring all {len(abstracts_to_score)} retrieved abstracts\")\n",
    "    \n",
    "    # Create async tasks for parallel execution\n",
    "    tasks = [\n",
    "        asyncio.create_task(\n",
    "            relevance_summary(\n",
    "                id=item['id'],\n",
    "                query=query,\n",
    "                reference_paper=item['title_abstract']\n",
    "            )\n",
    "        )\n",
    "        for index, item in abstracts_to_score[['id', 'title_abstract']].iterrows()\n",
    "    ]\n",
    "    \n",
    "    print(f\"Executing {len(tasks)} relevance scoring tasks in parallel...\")\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Parallel scoring function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "execute-scoring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring 3 abstracts (configured limit)\n",
      "Executing 3 relevance scoring tasks in parallel...\n",
      "\n",
      "✓ Completed scoring 3 abstracts\n",
      "CPU times: user 176 ms, sys: 49.6 ms, total: 226 ms\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Execute relevance scoring (handles both Jupyter notebook and async contexts)\n",
    "try:\n",
    "    # Try to get existing event loop (in Jupyter)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    if loop.is_running():\n",
    "        # If loop is already running (Jupyter), use nest_asyncio or create task\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        results = loop.run_until_complete(\n",
    "            gather_abstract_relevance(retrieved_abstracts, NUM_ABSTRACTS_TO_SCORE)\n",
    "        )\n",
    "    else:\n",
    "        results = loop.run_until_complete(\n",
    "            gather_abstract_relevance(retrieved_abstracts, NUM_ABSTRACTS_TO_SCORE)\n",
    "        )\n",
    "except RuntimeError:\n",
    "    # If no event loop exists, create one\n",
    "    results = asyncio.run(\n",
    "        gather_abstract_relevance(retrieved_abstracts, NUM_ABSTRACTS_TO_SCORE)\n",
    "    )\n",
    "\n",
    "print(f\"\\n✓ Completed scoring {len(results)} abstracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "scoring-stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Scoring Statistics:\n",
      "  - Papers scored: 3\n",
      "  - Mean score: 78.33\n",
      "  - Std dev: 9.43\n",
      "  - Min score: 65.00\n",
      "  - Max score: 85.00\n",
      "  - Median score: 85.00\n",
      "\n",
      "Sample relevance assessments:\n",
      "\n",
      "  Paper ID 34 (Score: 85.00):\n",
      "    For: The candidate reference paper explores the role of Retrieval-Augmented Generation (RAG) in Biomedica...\n",
      "    Against: The candidate paper emphasizes context and literature reshaping rather than proposing new strategies...\n",
      "\n",
      "  Paper ID 1 (Score: 85.00):\n",
      "    For: The candidate paper discusses a Retrieval-Augmented Generation (RAG) model specifically designed for...\n",
      "    Against: The candidate paper focuses on a more general application of RAG in scientific literature and does n...\n",
      "\n",
      "  Paper ID 50 (Score: 65.00):\n",
      "    For: The candidate paper discusses various tools and techniques for accessing biomedical literature, whic...\n",
      "    Against: The candidate abstract primarily focuses on access tools and systems for biomedical literature rathe...\n"
     ]
    }
   ],
   "source": [
    "# Display scoring statistics\n",
    "scores = [abs.probability_score for abs in results]\n",
    "\n",
    "print(\"Relevance Scoring Statistics:\")\n",
    "print(f\"  - Papers scored: {len(scores)}\")\n",
    "print(f\"  - Mean score: {np.mean(scores):.2f}\")\n",
    "print(f\"  - Std dev: {np.std(scores):.2f}\")\n",
    "print(f\"  - Min score: {np.min(scores):.2f}\")\n",
    "print(f\"  - Max score: {np.max(scores):.2f}\")\n",
    "print(f\"  - Median score: {np.median(scores):.2f}\")\n",
    "\n",
    "# Show sample of results\n",
    "print(f\"\\nSample relevance assessments:\")\n",
    "for i, result in enumerate(results[:3]):\n",
    "    print(f\"\\n  Paper ID {result.id} (Score: {result.probability_score:.2f}):\")\n",
    "    print(f\"    For: {result.arguments_for[:100]}...\")\n",
    "    print(f\"    Against: {result.arguments_against[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selection-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Top-K Selection\n",
    "\n",
    "Select the top-k most relevant papers based on their probability scores. These papers will be used to generate the \"Related Work\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "topk-function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Selected top 3 papers by relevance score\n"
     ]
    }
   ],
   "source": [
    "def get_top_k_abstracts(results: List[AbstractRelevance], k: int = 10) -> List[tuple]:\n",
    "    \"\"\"Select top-k papers by relevance score.\n",
    "    \n",
    "    Args:\n",
    "        results: List of AbstractRelevance objects\n",
    "        k: Number of top papers to select\n",
    "    \n",
    "    Returns:\n",
    "        List of (id, score) tuples sorted by score descending\n",
    "    \"\"\"\n",
    "    scores = [(abs.id, abs.probability_score) for abs in results]\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_scores[:k]\n",
    "\n",
    "# Get top-k papers\n",
    "top_k_scores = get_top_k_abstracts(results, k=TOP_K_PAPERS)\n",
    "\n",
    "print(f\"✓ Selected top {TOP_K_PAPERS} papers by relevance score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "display-topk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Papers Selected for Related Work:\n",
      "================================================================================\n",
      "\n",
      "[34] Score: 85.00\n",
      "Title: Reshaping Biomedical Scientific Literature in a RAG Pipeline for Question Answering\n",
      "Abstract: Biomedical Question Answering (BQA) poses specific challenges due to the specialized vocabulary and complex semantic structures of biomedical literature. Large Language Models (LLMs) have shown great ...\n",
      "\n",
      "[1] Score: 85.00\n",
      "Title: PaperQA: Retrieval-Augmented Generative Agent for Scientific Research\n",
      "Abstract: Large Language Models (LLMs) generalize well across language tasks, but suffer from hallucinations and uninterpretability, making it difficult to assess their accuracy without ground-truth. Retrieval-...\n",
      "\n",
      "[50] Score: 65.00\n",
      "Title: Accessing Biomedical Literature in the Current Information Landscape\n",
      "Abstract: Biomedical and life sciences literature is unique because of its exponentially increasing volume and interdisciplinary nature. Biomedical literature access is essential for several types of users incl...\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Reshaping Biomedical Scientific Literature in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>PaperQA: Retrieval-Augmented Generative Agent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Accessing Biomedical Literature in the Current...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  relevance_score                                              title\n",
       "33  34             85.0  Reshaping Biomedical Scientific Literature in ...\n",
       "0    1             85.0  PaperQA: Retrieval-Augmented Generative Agent ...\n",
       "49  50             65.0  Accessing Biomedical Literature in the Current..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract top-k paper IDs and get full information\n",
    "top_k_id = [id for id, score in top_k_scores]\n",
    "top_k_abstracts = retrieved_abstracts[retrieved_abstracts['id'].isin(top_k_id)].copy()\n",
    "\n",
    "# Add scores to DataFrame for display\n",
    "score_dict = {id: score for id, score in top_k_scores}\n",
    "top_k_abstracts['relevance_score'] = top_k_abstracts['id'].map(score_dict)\n",
    "top_k_abstracts = top_k_abstracts.sort_values('relevance_score', ascending=False)\n",
    "\n",
    "print(f\"Top {TOP_K_PAPERS} Papers Selected for Related Work:\")\n",
    "print(\"=\" * 80)\n",
    "for idx, row in top_k_abstracts.iterrows():\n",
    "    print(f\"\\n[{row['id']}] Score: {row['relevance_score']:.2f}\")\n",
    "    print(f\"Title: {row['title']}\")\n",
    "    print(f\"Abstract: {row['abstract'][:200]}...\")\n",
    "\n",
    "# Display as DataFrame\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "display(top_k_abstracts[['id', 'relevance_score', 'title']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generation-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Related Work Generation\n",
    "\n",
    "Generate a cohesive \"Related Work\" section using the top-k papers. The generation agent:\n",
    "- Creates a coherent narrative connecting the papers\n",
    "- Performs critical analysis comparing strengths and weaknesses\n",
    "- Motivates the proposed approach in context of prior work\n",
    "- Cites papers using [id] format\n",
    "- Avoids copying abstracts verbatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "generation-prompt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generation instructions defined\n"
     ]
    }
   ],
   "source": [
    "# Define instructions for related work generation\n",
    "INSTRUCTIONS_RELATED_WORK = \"\"\" \n",
    "You are an expert research assistant who is helping with literature review for a research idea or abstract. \n",
    "You will be provided with an abstract or research idea and a list of reference abstracts. \n",
    "Your task is to write the related work section of the document using only the provided reference abstracts. \n",
    "Please write the related work section creating a cohesive storyline by doing a critical analysis of prior work \n",
    "in the reference abstracts comparing the strengths and weaknesses while also motivating the proposed approach. \n",
    "You should cite the reference abstracts as [id] whenever you are referring it in the related work. \n",
    "Do not write it as Reference #. Do not cite abstract or research Idea. \n",
    "Do not include any extra notes or newline characters at the end. \n",
    "Do not copy the abstracts of reference papers directly but compare and contrast to the main work concisely. \n",
    "Do not provide the output in bullet points or markdown. \n",
    "Do not provide references at the end. \n",
    "Please cite all the provided reference papers if needed.\n",
    "\"\"\"\n",
    "\n",
    "print(\"✓ Generation instructions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "build-generation-input",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Built generation input (4758 characters)\n"
     ]
    }
   ],
   "source": [
    "# Build input for related work generation\n",
    "input_related_work = f\"Given the Research Idea or abstract: {query}\"\n",
    "input_related_work += \"\\n\\n## Given references abstracts list below:\"\n",
    "\n",
    "for index, item in top_k_abstracts[['id', 'title_abstract']].iterrows():\n",
    "    input_related_work += f\"\\n\\n[{item['id']}]: {item['title_abstract']}\"\n",
    "\n",
    "input_related_work += \"\\n\\nWrite the related work section summarizing in a cohesive story prior works relevant to the research idea.\"\n",
    "input_related_work += \"\\n\\n## Related Work:\"\n",
    "\n",
    "print(f\"✓ Built generation input ({len(input_related_work)} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "generate-related-work",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Related work section generated\n",
      "  Length: 2854 characters\n",
      "  Words: ~376 words\n",
      "CPU times: user 7.92 ms, sys: 3.81 ms, total: 11.7 ms\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Generate related work section\n",
    "response = openai_client.responses.create(\n",
    "    model=GENERATION_MODEL,\n",
    "    instructions=INSTRUCTIONS_RELATED_WORK,\n",
    "    input=input_related_work\n",
    ")\n",
    "\n",
    "generated_related_work = response.output_text\n",
    "\n",
    "print(\"✓ Related work section generated\")\n",
    "print(f\"  Length: {len(generated_related_work)} characters\")\n",
    "print(f\"  Words: ~{len(generated_related_work.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Results & Evaluation\n",
    "\n",
    "Display the final generated \"Related Work\" section with formatting and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "display-results",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Generated Related Work Section"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Retrieval-augmented generation (RAG) systems have recently garnered attention for their potential to tackle the inherent challenges of biomedical question answering (BQA), a domain characterized by specialized terminology and intricate semantic structures. A significant advancement in this area is highlighted in the research examining the role of structured context in RAG pipelines. This work emphasizes that proper literature reshaping can significantly enhance the quality of generated responses, particularly improving precision metrics, although it suggests a lesser impact on recall [34]. The findings underscore the necessity of fine-tuning context in RAG applications, an aspect that remains critical for effective communication of complex biomedical information.\n",
       "\n",
       "Conversely, other studies, such as the implementation of a retrieval-augmented generative agent named PaperQA, showcase a broader ambition to streamline the integration of scientific literature into question-answering systems. By utilizing full-text articles and assessing the relevance of numerous sources, PaperQA not only mitigates issues related to hallucination but also surpasses the performance of traditional large language models (LLMs) on established science QA benchmarks [1]. This contrasts with the domain-specific focus noted in the previous work, as PaperQA aims to replicate human-like research capabilities by synthesizing information across a wider array of scientific sources. This capability might be pivotal in expanding access to evidence-based practices in biomedical literature, highlighting the gap in user accessibility raised in other investigations [50].\n",
       "\n",
       "Additionally, the broader landscape of biomedical literature access delineates the challenges users face when navigating vast databases, including variations in search engines and retrieval methodologies [50]. While several tools have emerged to improve user experience, they address only parts of the information retrieval process. This delineation aligns with the proposed strategy that emphasizes creating an RAG framework specifically tailored for biomedical contexts, thereby directly addressing inaccuracies and the complexities of user queries. \n",
       "\n",
       "In summary, while the existing literature on RAG systems provides substantial groundwork, particularly in context enhancement and performance evaluation, gaps remain regarding their generalizability and user accessibility. The proposed approach seeks to bridge these gaps by leveraging the strengths of previous frameworks while addressing their limitations, ultimately facilitating wider public and professional access to crucial biomedical knowledge. This direction is crucial not only for researchers but also for public health professionals who require clear and precise information amidst the vast complexities of biomedical literature."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the generated related work section\n",
    "display(Markdown(\"## Generated Related Work Section\"))\n",
    "display(Markdown(\"---\"))\n",
    "display(Markdown(generated_related_work))\n",
    "display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "citations-used",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citations Used in Generated Text:\n",
      "  - Total citations: 4\n",
      "  - Unique papers cited: 3\n",
      "  - Papers provided: 3\n",
      "  - Citation IDs: [1, 34, 50]\n",
      "\n",
      "Cited Papers:\n",
      "  [1] PaperQA: Retrieval-Augmented Generative Agent for Scientific Research\n",
      "  [34] Reshaping Biomedical Scientific Literature in a RAG Pipeline for Question Answering\n",
      "  [50] Accessing Biomedical Literature in the Current Information Landscape\n"
     ]
    }
   ],
   "source": [
    "# Extract citations used in the generated text\n",
    "import re\n",
    "\n",
    "citations = re.findall(r'\\[(\\d+)\\]', generated_related_work)\n",
    "unique_citations = sorted(set(int(c) for c in citations))\n",
    "\n",
    "print(f\"Citations Used in Generated Text:\")\n",
    "print(f\"  - Total citations: {len(citations)}\")\n",
    "print(f\"  - Unique papers cited: {len(unique_citations)}\")\n",
    "print(f\"  - Papers provided: {len(top_k_abstracts)}\")\n",
    "print(f\"  - Citation IDs: {unique_citations}\")\n",
    "\n",
    "# Show which papers were cited\n",
    "print(f\"\\nCited Papers:\")\n",
    "for paper_id in unique_citations:\n",
    "    paper = top_k_abstracts[top_k_abstracts['id'] == paper_id]\n",
    "    if not paper.empty:\n",
    "        print(f\"  [{paper_id}] {paper.iloc[0]['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "pipeline-summary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Pipeline Execution Summary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Configuration:**\n",
       "- Corpus size: 78 papers\n",
       "- Hybrid retrieval: Top 50 papers\n",
       "- Papers retrieved: 20 unique papers\n",
       "- Papers scored: 3 papers\n",
       "- Top-K selection: 3 papers\n",
       "- Papers cited in output: 3 papers\n",
       "\n",
       "**Models Used:**\n",
       "- Relevance scoring: gpt-4o-mini\n",
       "- Related work generation: gpt-4o-mini\n",
       "\n",
       "**Output:**\n",
       "- Related work length: 2854 characters (~376 words)\n",
       "- Citations included: 4 total, 3 unique\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pipeline execution summary\n",
    "display(Markdown(\"## Pipeline Execution Summary\"))\n",
    "\n",
    "summary = f\"\"\"\n",
    "**Configuration:**\n",
    "- Corpus size: {len(all_abstracts)} papers\n",
    "- Hybrid retrieval: Top {HYBRID_SEARCH_K} papers\n",
    "- Papers retrieved: {len(retrieved_abstracts)} unique papers\n",
    "- Papers scored: {len(results)} papers\n",
    "- Top-K selection: {TOP_K_PAPERS} papers\n",
    "- Papers cited in output: {len(unique_citations)} papers\n",
    "\n",
    "**Models Used:**\n",
    "- Relevance scoring: {RELEVANCE_MODEL}\n",
    "- Related work generation: {GENERATION_MODEL}\n",
    "\n",
    "**Output:**\n",
    "- Related work length: {len(generated_related_work)} characters (~{len(generated_related_work.split())} words)\n",
    "- Citations included: {len(citations)} total, {len(unique_citations)} unique\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "save-output",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Output saved to generated_related_work.txt\n"
     ]
    }
   ],
   "source": [
    "# Optional: Save the generated related work to a file\n",
    "SAVE_OUTPUT = True  # Set to True to save\n",
    "\n",
    "if SAVE_OUTPUT:\n",
    "    output_file = \"generated_related_work.txt\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"RESEARCH QUERY:\\n\")\n",
    "        f.write(query)\n",
    "        f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "        f.write(\"RELATED WORK:\\n\")\n",
    "        f.write(generated_related_work)\n",
    "        f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "        f.write(\"REFERENCES:\\n\")\n",
    "        for paper_id in unique_citations:\n",
    "            paper = top_k_abstracts[top_k_abstracts['id'] == paper_id]\n",
    "            if not paper.empty:\n",
    "                f.write(f\"[{paper_id}] {paper.iloc[0]['title']}\\n\")\n",
    "    \n",
    "    print(f\"✓ Output saved to {output_file}\")\n",
    "else:\n",
    "    print(\"Output not saved (set SAVE_OUTPUT=True to save)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated an end-to-end agentic RAG pipeline for automated literature review generation. Key features:\n",
    "\n",
    "1. **Hybrid Retrieval**: Combines semantic and keyword search for comprehensive coverage\n",
    "2. **Agentic Scoring**: Uses structured reasoning (debate-style) for reliable relevance assessment\n",
    "3. **Parallel Processing**: Efficiently scores multiple papers concurrently\n",
    "4. **Coherent Synthesis**: Generates well-structured literature reviews with proper citations\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with different retrieval parameters (`HYBRID_SEARCH_K`)\n",
    "- Adjust the number of papers to score (`NUM_ABSTRACTS_TO_SCORE`)\n",
    "- Try different top-k values (`TOP_K_PAPERS`)\n",
    "- Evaluate different LLM models for scoring and generation\n",
    "- Expand the corpus with more biomedical abstracts\n",
    "- Add evaluation metrics for generated related work quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-lit-llm (3.13.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
