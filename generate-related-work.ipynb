{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Automated Literature Review Generation using Agentic RAG\n",
    "\n",
    "This notebook demonstrates an end-to-end pipeline for automatically generating \"Related Work\" sections for scientific papers using:\n",
    "- **Hybrid Retrieval**: Combining semantic search (vector embeddings) and keyword search (BM25)\n",
    "- **Agentic Relevance Scoring**: Using LLM agents to evaluate paper relevance with structured reasoning\n",
    "- **Automated Synthesis**: Generating coherent literature reviews with proper citations\n",
    "\n",
    "## Pipeline Overview\n",
    "1. Load biomedical abstracts corpus\n",
    "2. Create vector store with hybrid retrieval capabilities\n",
    "3. Define research query/abstract\n",
    "4. Retrieve candidate papers using hybrid search\n",
    "5. Score papers using debate-style relevance agent\n",
    "6. Select top-k most relevant papers\n",
    "7. Generate cohesive \"Related Work\" section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Configuration\n",
    "\n",
    "Import dependencies and configure pipeline parameters. Adjust these settings to customize the pipeline behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "autoreload",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-reload modules for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import asyncio\n",
    "from typing import List, Dict, Any\n",
    "from pprint import pprint\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# OpenAI and agents\n",
    "import openai\n",
    "from agents import Agent, Runner\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated\n",
    "\n",
    "# Environment and display\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, HTML\n",
    "\n",
    "# Local modules\n",
    "from vectorstore import VectorStoreAbstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "configuration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n",
      "  - Retrieval: Top 50 papers using hybrid search\n",
      "  - Scoring: 3 abstracts will be scored\n",
      "  - Selection: Top 3 papers for related work\n",
      "  - Models: gpt-4o-mini (scoring), gpt-4o-mini (generation)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Vector Store Configuration\n",
    "CHROMA_PERSIST_DIRECTORY = \"./corpus-data/chroma_db\"\n",
    "RECREATE_INDEX = False  # Set to True to rebuild the index from scratch\n",
    "\n",
    "# Retrieval Configuration\n",
    "HYBRID_SEARCH_K = 50  # Number of papers to retrieve using hybrid search\n",
    "\n",
    "# Relevance Scoring Configuration\n",
    "NUM_ABSTRACTS_TO_SCORE = 3  # Set to None to score all retrieved abstracts, or set a number for testing (e.g., 5, 10, 20)\n",
    "RELEVANCE_MODEL = \"gpt-4o-mini\"  # Model for relevance scoring agent\n",
    "\n",
    "# Top-K Selection Configuration\n",
    "TOP_K_PAPERS = 3  # Number of top-ranked papers to include in related work\n",
    "\n",
    "# Related Work Generation Configuration\n",
    "GENERATION_MODEL = \"gpt-4o-mini\"  # Model for generating related work section\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"  - Retrieval: Top {HYBRID_SEARCH_K} papers using hybrid search\")\n",
    "print(f\"  - Scoring: {'All' if NUM_ABSTRACTS_TO_SCORE is None else NUM_ABSTRACTS_TO_SCORE} abstracts will be scored\")\n",
    "print(f\"  - Selection: Top {TOP_K_PAPERS} papers for related work\")\n",
    "print(f\"  - Models: {RELEVANCE_MODEL} (scoring), {GENERATION_MODEL} (generation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "env-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment loaded\n",
      "✓ OpenAI client initialized\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables and initialize OpenAI client\n",
    "load_dotenv(override=True)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_client = openai.OpenAI()\n",
    "\n",
    "print(\"✓ Environment loaded\")\n",
    "print(\"✓ OpenAI client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Loading & Preparation\n",
    "\n",
    "Load the biomedical abstracts corpus and prepare it for indexing. Each abstract contains:\n",
    "- **id**: Unique identifier\n",
    "- **title**: Paper title\n",
    "- **abstract**: Paper abstract\n",
    "- **title_abstract**: Concatenated title and abstract for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78 abstracts from corpus\n",
      "\n",
      "Dataset columns: ['id', 'title', 'abstract']\n",
      "\n",
      "First few abstracts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>Reshaping Biomedical Scientific Literature in ...</td>\n",
       "      <td>Biomedical Question Answering (BQA) poses spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PaperQA: Retrieval-Augmented Generative Agent ...</td>\n",
       "      <td>Large Language Models (LLMs) generalize well a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>Attention is all you need, A</td>\n",
       "      <td>The dominant sequence transduction models are ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                              title  \\\n",
       "33  34  Reshaping Biomedical Scientific Literature in ...   \n",
       "0    1  PaperQA: Retrieval-Augmented Generative Agent ...   \n",
       "34  35                       Attention is all you need, A   \n",
       "\n",
       "                                             abstract  \n",
       "33  Biomedical Question Answering (BQA) poses spec...  \n",
       "0   Large Language Models (LLMs) generalize well a...  \n",
       "34  The dominant sequence transduction models are ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load abstracts from CSV and shuffle\n",
    "all_abstracts = pd.read_csv('./abstracts_rag.csv').sample(frac=1, random_state=42)\n",
    "\n",
    "print(f\"Loaded {len(all_abstracts)} abstracts from corpus\")\n",
    "print(f\"\\nDataset columns: {list(all_abstracts.columns)}\")\n",
    "print(f\"\\nFirst few abstracts:\")\n",
    "all_abstracts.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "prepare-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prepared 78 abstracts for indexing\n",
      "\n",
      "Sample abstract structure:\n",
      "  - ID: 34\n",
      "  - Text length: 1325 characters\n"
     ]
    }
   ],
   "source": [
    "# Concatenate title and abstract for better retrieval\n",
    "all_abstracts['title_abstract'] = all_abstracts['title'] + all_abstracts['abstract']\n",
    "\n",
    "# Convert to list of dictionaries for vector store\n",
    "samples_abstracts = [\n",
    "    v for k, v in all_abstracts[['title_abstract', 'id']].reset_index(drop=True).T.to_dict().items()\n",
    "]\n",
    "\n",
    "print(f\"✓ Prepared {len(samples_abstracts)} abstracts for indexing\")\n",
    "print(f\"\\nSample abstract structure:\")\n",
    "print(f\"  - ID: {samples_abstracts[0]['id']}\")\n",
    "print(f\"  - Text length: {len(samples_abstracts[0]['title_abstract'])} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vectorstore-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Vector Store Initialization\n",
    "\n",
    "Initialize ChromaDB vector store with hybrid retrieval capabilities:\n",
    "- **Semantic Search**: Uses HuggingFace embeddings (all-MiniLM-L6-v2)\n",
    "- **Keyword Search**: Uses BM25 algorithm\n",
    "- **Chunking**: Splits abstracts into 150-character chunks with 20-character overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "init-vectorstore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new index at ./corpus-data/chroma_db\n",
      "✓ Created new index at ./corpus-data/chroma_db\n"
     ]
    }
   ],
   "source": [
    "# Initialize vector store\n",
    "vector_store = VectorStoreAbstract(\n",
    "    abstracts=samples_abstracts,\n",
    "    persist_directory=CHROMA_PERSIST_DIRECTORY,\n",
    "    recreate_index=RECREATE_INDEX\n",
    ")\n",
    "\n",
    "# Display index status\n",
    "if vector_store.index_exists:\n",
    "    doc_count = vector_store.get_document_count()\n",
    "    print(f\"✓ Using existing index at {CHROMA_PERSIST_DIRECTORY}\")\n",
    "    print(f\"  Index contains {doc_count} document chunks\")\n",
    "else:\n",
    "    print(f\"✓ Created new index at {CHROMA_PERSIST_DIRECTORY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "chunk-documents",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking documents: 100%|██████████| 78/78 [00:00<00:00, 6881.40article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 1138 document chunks\n",
      "CPU times: user 9.93 ms, sys: 15.6 ms, total: 25.5 ms\n",
      "Wall time: 25.2 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Chunk documents if needed (only when creating new index or recreating)\n",
    "if vector_store.should_process_documents():\n",
    "    print(\"Chunking documents...\")\n",
    "    documents = vector_store.chunking()\n",
    "    print(f\"✓ Created {len(documents)} document chunks\")\n",
    "else:\n",
    "    print(\"✓ Skipping document chunking (using existing index)\")\n",
    "    documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "index-documents",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing 1138 documents (this may take several minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings: 100%|██████████| 1138/1138 [00:02<00:00, 413.27doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Indexing completed!\n",
      "  Total chunks indexed: 0\n",
      "CPU times: user 1.5 s, sys: 546 ms, total: 2.04 s\n",
      "Wall time: 2.77 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Index documents if needed\n",
    "if vector_store.should_process_documents():\n",
    "    print(f\"Indexing {len(documents)} documents (this may take several minutes)...\")\n",
    "    vector_store.index_document(documents)\n",
    "    print(\"✓ Indexing completed!\")\n",
    "    print(f\"  Total chunks indexed: {vector_store.get_document_count()}\")\n",
    "else:\n",
    "    print(\"✓ Skipping document indexing (using existing index)\")\n",
    "    print(f\"  Ready to perform searches!\")\n",
    "    print(f\"  Index contains {vector_store.get_document_count()} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Research Query Definition\n",
    "\n",
    "Define the research query or abstract for which we want to generate a literature review. This will be used to:\n",
    "1. Retrieve relevant papers from the corpus\n",
    "2. Score the relevance of each retrieved paper\n",
    "3. Generate the final \"Related Work\" section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "define-query",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Research Query/Abstract"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "_Retrieval-augmented generation (RAG) systems are emerging as effective tools for biomedical literature. \n",
       "However, their performance in this domain is not yet generalizable. \n",
       "We propose a new strategy for high-performing RAG applied to biomedical question answering. \n",
       "This approach would allow the wider public and public health professionals to access evidence from scientific literature in easy-to-understand language._"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query length: 419 characters\n"
     ]
    }
   ],
   "source": [
    "# Define the research query/abstract\n",
    "query = \"\"\"\n",
    "Retrieval-augmented generation (RAG) systems are emerging as effective tools for biomedical literature. \n",
    "However, their performance in this domain is not yet generalizable. \n",
    "We propose a new strategy for high-performing RAG applied to biomedical question answering. \n",
    "This approach would allow the wider public and public health professionals to access evidence from scientific literature in easy-to-understand language.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Display the query\n",
    "display(Markdown(\"### Research Query/Abstract\"))\n",
    "display(Markdown(f\"_{query}_\"))\n",
    "print(f\"\\nQuery length: {len(query)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retrieval-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Hybrid Retrieval\n",
    "\n",
    "Perform hybrid search combining:\n",
    "- **Semantic similarity**: Vector search using embeddings\n",
    "- **Keyword matching**: BM25 ranking\n",
    "\n",
    "The ensemble retriever combines both methods with equal weights (0.5, 0.5) to balance semantic understanding and keyword relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hybrid-search",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='8f729245-0bcb-434b-92d9-12b140858776', metadata={'id': 23}, page_content='. Here we explored the use of a retrieval-augmented generation (RAG) model which we tested on literature specific to a biomedical research area'), Document(id='39229b77-e288-4148-b603-203e56953712', metadata={'id': 33}, page_content='. The findings underscore the potential of RAG-enhanced language models to bridge the gap between complex biomedical literature and accessible public'), Document(id='675b284d-8d0b-405c-b775-bf634b3ce1be', metadata={'id': 33}, page_content='Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)This work presents a Biomedical Literature Question Answering (Q&A) system'), Document(metadata={'id': 33}, page_content='. Addressing the shortcomings of conventional health search engines and the lag in public access to biomedical research'), Document(id='8f6fdb73-dc9f-4e12-867a-3f3ac97232da', metadata={'id': 32}, page_content='RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical Question AnsweringThe exponential growth of biomedical literature creates'), Document(metadata={'id': 33}, page_content='accessible public health knowledge'), Document(metadata={'id': 41}, page_content='. However, in the biomedical domain'), Document(id='a0a1442c-66b7-466f-be6e-624c8a5bdd26', metadata={'id': 32}, page_content='. We present RAG-BioQA, a novel framework combining retrieval-augmented generation with domain-specific fine-tuning to produce evidence-based'), Document(metadata={'id': 49}, page_content='. We recruited fifteen professionals from various biomedical roles and industries to participate in sixty-minute semi-structured interviews'), Document(id='38106e44-3100-42da-b31b-1cf7e7e4ca24', metadata={'id': 38}, page_content='Enhancing biomedical question answering with parameter-efficient finetuning and hierarchical retrieval augmented generation'), Document(metadata={'id': 49}, page_content='. We applied a qualitative analysis of individual interviews using an inductive-deductive thematic coding approach for emerging themes'), Document(id='98f45a68-b13c-4509-ab84-818bd876b802', metadata={'id': 34}, page_content='Reshaping Biomedical Scientific Literature in a RAG Pipeline for Question AnsweringBiomedical Question Answering (BQA) poses specific challenges due'), Document(id='45d3664e-6b9a-4b9d-a702-4e37b4f984fa', metadata={'id': 56}, page_content=', up-to-date information via retrieval-augmented generation (RAG)'), Document(metadata={'id': 18}, page_content='attention in the biomedical domain'), Document(id='80d7acc4-f943-4a5f-82f5-c5b2392ef684', metadata={'id': 70}, page_content='LightRAG: Simple and Fast Retrieval-Augmented GenerationRetrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by'), Document(metadata={'id': 1}, page_content='. We present PaperQA, a RAG agent for answering questions over the scientific literature'), Document(id='bacfb9de-17cd-4431-aeac-12dd46a6fcfb', metadata={'id': 2}, page_content=', particularly when employing Retrieval Augmented Generation (RAG) techniques'), Document(metadata={'id': 19}, page_content='shift from general domain corpora to biomedical corpora'), Document(id='9238bbec-25da-42e9-9ceb-5a33e0d81ef1', metadata={'id': 3}, page_content='Retrieval augmented generation for large language models in healthcare: A systematic reviewLarge Language Models (LLMs) have demonstrated promising'), Document(metadata={'id': 50}, page_content='. Biomedical literature access is essential for several types of users including biomedical researchers, clinicians, database curators'), Document(id='3d0b8806-b6db-4cbd-8b6e-1b8412500d0e', metadata={'id': 38}, page_content=', which indicate that PEFT and RAG can significantly improve the performance in biomedical Question Answering (QA) tasks'), Document(metadata={'id': 50}, page_content='. Finally, the last section describes some predicted future trends for improving biomedical literature access'), Document(id='6f813ba4-6a7b-443e-ab44-12cd7c3714f2', metadata={'id': 66}, page_content='. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases'), Document(metadata={'id': 7}, page_content='SciBERT: A pretrained language model for scientific textObtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and'), Document(id='d42896f7-eaf5-451b-bfbd-f9875d35e7e0', metadata={'id': 56}, page_content='RadioRAG: Online Retrieval-Augmented Generation for Radiology Question AnsweringPurpose To evaluate diagnostic accuracy of various large language'), Document(metadata={'id': 71}, page_content='. This is evidence that generative models are good at aggregating and combining evidence from multiple passages.'), Document(id='00913f01-6098-42b4-a875-98f31a4e53d2', metadata={'id': 2}, page_content='Improving Retrieval for RAG based Question Answering Models on Financial DocumentsThe effectiveness of Large Language Models (LLMs) in generating'), Document(id='cdf07892-ab45-4f1c-a46c-5c72e62bf681', metadata={'id': 33}, page_content='(Q&A) system based on a Retrieval-Augmented Generation (RAG) architecture'), Document(metadata={'id': 3}, page_content='. It is important to account for ethical challenges that are inherent when AI systems are implemented in the clinical setting'), Document(id='f8365dca-0752-4bbf-b634-28f5f686195c', metadata={'id': 53}, page_content='. Retrieval Augmented Generation (RAG) enables customization by integrating specialized knowledge'), Document(metadata={'id': 24}, page_content='. Our results demonstrate the potential for large language models to be effective tools in the clinical decision-making process'), Document(metadata={'id': 19}, page_content='Biobert: a pre-trained biomedical language representation model for biomedical text miningBiomedical text mining is becoming increasingly important as'), Document(id='92dc862f-b898-4e45-8872-ead51405071d', metadata={'id': 57}, page_content=', a Retrieval-Augmented Generation (RAG)-based AI assistant designed to deliver automated and clinically grounded responses to frequently asked'), Document(metadata={'id': 18}, page_content='. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for'), Document(id='7a842b85-2bae-4d2b-b204-577166a0e7f8', metadata={'id': 41}, page_content='Incorporating entity-level knowledge in pretrained language model for biomedical dense retrievalIn recent years'), Document(metadata={'id': 8}, page_content='are tools used by the academic community for research and research evaluation that aggregate scientific literature output and measure impact by'), Document(id='78bfe1fd-2e63-4279-93a7-d765dccf1240', metadata={'id': 1}, page_content='PaperQA: Retrieval-Augmented Generative Agent for Scientific ResearchLarge Language Models (LLMs) generalize well across language tasks'), Document(metadata={'id': 51}, page_content='. Retrieval-augmented generation (RAG) is an enterprise architecture that allows the embedding of customized data into LLMs')]\n",
      "✓ Retrieved 23 unique papers (from 50 chunks)\n",
      "\n",
      "Top 5 retrieved papers:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>Reshaping Biomedical Scientific Literature in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PaperQA: Retrieval-Augmented Generative Agent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>Accessing Biomedical Literature in the Current...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Improving accuracy of gpt-3/4 results on biome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Biobert: a pre-trained biomedical language rep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                              title\n",
       "33  34  Reshaping Biomedical Scientific Literature in ...\n",
       "0    1  PaperQA: Retrieval-Augmented Generative Agent ...\n",
       "49  50  Accessing Biomedical Literature in the Current...\n",
       "22  23  Improving accuracy of gpt-3/4 results on biome...\n",
       "18  19  Biobert: a pre-trained biomedical language rep..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Perform hybrid search\n",
    "rs = vector_store.hybrid_search(query, k=HYBRID_SEARCH_K)\n",
    "#rs = vector_store.semantic_search(query, k=HYBRID_SEARCH_K)\n",
    "\n",
    "print(rs)\n",
    "\n",
    "# Extract unique document IDs from results\n",
    "retrieved_docs = {item.metadata['id'] for item in rs}\n",
    "\n",
    "# Filter abstracts DataFrame to get full information for retrieved papers\n",
    "retrieved_abstracts = all_abstracts[all_abstracts['id'].isin(retrieved_docs)].copy()\n",
    "\n",
    "print(f\"✓ Retrieved {len(retrieved_abstracts)} unique papers (from {HYBRID_SEARCH_K} chunks)\")\n",
    "print(f\"\\nTop 5 retrieved papers:\")\n",
    "display(retrieved_abstracts[['id', 'title']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "retrieval-stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Statistics:\n",
      "  - Total papers in corpus: 78\n",
      "  - Papers retrieved: 23\n",
      "  - Retrieval rate: 29.5%\n",
      "\n",
      "Sample retrieved abstracts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>Reshaping Biomedical Scientific Literature in ...</td>\n",
       "      <td>Biomedical Question Answering (BQA) poses spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PaperQA: Retrieval-Augmented Generative Agent ...</td>\n",
       "      <td>Large Language Models (LLMs) generalize well a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>Accessing Biomedical Literature in the Current...</td>\n",
       "      <td>Biomedical and life sciences literature is uni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                              title  \\\n",
       "33  34  Reshaping Biomedical Scientific Literature in ...   \n",
       "0    1  PaperQA: Retrieval-Augmented Generative Agent ...   \n",
       "49  50  Accessing Biomedical Literature in the Current...   \n",
       "\n",
       "                                             abstract  \n",
       "33  Biomedical Question Answering (BQA) poses spec...  \n",
       "0   Large Language Models (LLMs) generalize well a...  \n",
       "49  Biomedical and life sciences literature is uni...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display retrieval statistics\n",
    "print(f\"Retrieval Statistics:\")\n",
    "print(f\"  - Total papers in corpus: {len(all_abstracts)}\")\n",
    "print(f\"  - Papers retrieved: {len(retrieved_abstracts)}\")\n",
    "print(f\"  - Retrieval rate: {len(retrieved_abstracts) / len(all_abstracts) * 100:.1f}%\")\n",
    "print(f\"\\nSample retrieved abstracts:\")\n",
    "display(retrieved_abstracts[['id', 'title', 'abstract']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Relevance Agent Setup\n",
    "\n",
    "Configure the relevance scoring agent that evaluates each paper using a debate-style approach:\n",
    "1. Generate arguments **for** including the paper\n",
    "2. Generate arguments **against** including the paper\n",
    "3. Extract supporting quotes from the abstract\n",
    "4. Assign a relevance probability score (1-100)\n",
    "\n",
    "This structured reasoning helps ensure high-quality relevance judgments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "define-relevance-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ AbstractRelevance model defined\n"
     ]
    }
   ],
   "source": [
    "# Define the structured output model for relevance scoring\n",
    "class AbstractRelevance(BaseModel):\n",
    "    \"\"\"Structured relevance assessment for a candidate paper.\"\"\"\n",
    "    id: int\n",
    "    arguments_for: str\n",
    "    arguments_for_quotes: list[str]\n",
    "    arguments_against: str\n",
    "    arguments_against_quotes: list[str]\n",
    "    probability_score: Annotated[\n",
    "        float, \n",
    "        Field(ge=1.0, le=100.0, description=\"A relevance score between 1 and 100.\")\n",
    "    ]\n",
    "\n",
    "print(\"✓ AbstractRelevance model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "create-agent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Relevance agent factory created\n"
     ]
    }
   ],
   "source": [
    "def create_relevance_agent():\n",
    "    \"\"\"Create an agent that scores paper relevance using debate-style reasoning.\"\"\"\n",
    "    \n",
    "    INSTRUCTIONS_DEBATE_RANKING = \"\"\" \n",
    "    You are a helpful research assistant who is helping with literature review of a research idea. \n",
    "    You will be given a query or research idea and a candidate reference abstract.\n",
    "    Your task is to score reference abstract based on their relevance to the query. Please make sure you read and understand these instructions carefully. \n",
    "    Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "    ## Instruction: \n",
    "    Use the following steps to rank the reference papers:\n",
    "\n",
    "    1. Generate arguments for including this reference abstract in the literature review.\n",
    "\n",
    "    2. Generate arguments against including this reference abstract in the literature review.\n",
    "\n",
    "    3. Extract relevant sentences from the candidate paper abstract to support each argument.\n",
    "\n",
    "    4. Then, provide a score between 1 and 100 (up to two decimal places) that is proportional to the probability \n",
    "    of a paper with the given query including the candidate reference paper in its literature review. \n",
    "\n",
    "    Important:\n",
    "    - Put the extracted sentences in quotes\n",
    "    - You can use the information in other candidate papers when generating the arguments for a candidate paper\n",
    "    - Generate arguments and probability for each paper separately\n",
    "    - Do not generate anything else apart from the probability and the arguments\n",
    "    - Follow this process even if a candidate paper happens to be identical or near-perfect match to the query abstract\n",
    "\n",
    "    Your Response: \"\"\"\n",
    "\n",
    "    relevance_agent = Agent(\n",
    "        name=\"RelevanceAgent\",\n",
    "        instructions=INSTRUCTIONS_DEBATE_RANKING,\n",
    "        model=RELEVANCE_MODEL,\n",
    "        output_type=AbstractRelevance\n",
    "    )\n",
    "    \n",
    "    return relevance_agent\n",
    "\n",
    "print(\"✓ Relevance agent factory created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "relevance-function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Relevance scoring function defined\n"
     ]
    }
   ],
   "source": [
    "async def relevance_summary(id: int, query: str, reference_paper: str) -> AbstractRelevance:\n",
    "    \"\"\"Score a single paper's relevance to the query using the relevance agent.\n",
    "    \n",
    "    Args:\n",
    "        id: Paper ID\n",
    "        query: Research query/abstract\n",
    "        reference_paper: Candidate paper's title and abstract\n",
    "    \n",
    "    Returns:\n",
    "        AbstractRelevance object with scoring and reasoning\n",
    "    \"\"\"\n",
    "    relevance_agent = create_relevance_agent()\n",
    "    \n",
    "    user_instructions = f\"\"\"\n",
    "For this query abstract with id={id}\n",
    "\n",
    "Given the query abstract: {query}\n",
    "\n",
    "Given the candidate reference paper abstract: {reference_paper}\n",
    "\n",
    "Your Reference Abstract Relevance:\n",
    "\"\"\"\n",
    "    \n",
    "    result = await Runner.run(relevance_agent, input=user_instructions)\n",
    "    return result.final_output\n",
    "\n",
    "print(\"✓ Relevance scoring function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scoring-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Parallel Relevance Scoring\n",
    "\n",
    "Score all retrieved papers in parallel using async execution for efficiency. Each paper is evaluated independently by the relevance agent.\n",
    "\n",
    "**Note**: Adjust `NUM_ABSTRACTS_TO_SCORE` in the configuration section to limit the number of papers scored (useful for testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "parallel-scoring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Parallel scoring function defined\n"
     ]
    }
   ],
   "source": [
    "async def gather_abstract_relevance(retrieved_abstracts: pd.DataFrame, num_to_score: int = None) -> List[AbstractRelevance]:\n",
    "    \"\"\"Score multiple abstracts in parallel.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_abstracts: DataFrame of retrieved papers\n",
    "        num_to_score: Number of abstracts to score (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        List of AbstractRelevance objects\n",
    "    \"\"\"\n",
    "    # Select subset if specified\n",
    "    if num_to_score is not None:\n",
    "        abstracts_to_score = retrieved_abstracts.head(num_to_score)\n",
    "        print(f\"Scoring {num_to_score} abstracts (configured limit)\")\n",
    "    else:\n",
    "        abstracts_to_score = retrieved_abstracts\n",
    "        print(f\"Scoring all {len(abstracts_to_score)} retrieved abstracts\")\n",
    "    \n",
    "    # Create async tasks for parallel execution\n",
    "    tasks = [\n",
    "        asyncio.create_task(\n",
    "            relevance_summary(\n",
    "                id=item['id'],\n",
    "                query=query,\n",
    "                reference_paper=item['title_abstract']\n",
    "            )\n",
    "        )\n",
    "        for index, item in abstracts_to_score[['id', 'title_abstract']].iterrows()\n",
    "    ]\n",
    "    \n",
    "    print(f\"Executing {len(tasks)} relevance scoring tasks in parallel...\")\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Parallel scoring function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "execute-scoring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring 3 abstracts (configured limit)\n",
      "Executing 3 relevance scoring tasks in parallel...\n",
      "\n",
      "✓ Completed scoring 3 abstracts\n",
      "CPU times: user 156 ms, sys: 38.4 ms, total: 194 ms\n",
      "Wall time: 6.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Execute relevance scoring (handles both Jupyter notebook and async contexts)\n",
    "try:\n",
    "    # Try to get existing event loop (in Jupyter)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    if loop.is_running():\n",
    "        # If loop is already running (Jupyter), use nest_asyncio or create task\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        results = loop.run_until_complete(\n",
    "            gather_abstract_relevance(retrieved_abstracts, NUM_ABSTRACTS_TO_SCORE)\n",
    "        )\n",
    "    else:\n",
    "        results = loop.run_until_complete(\n",
    "            gather_abstract_relevance(retrieved_abstracts, NUM_ABSTRACTS_TO_SCORE)\n",
    "        )\n",
    "except RuntimeError:\n",
    "    # If no event loop exists, create one\n",
    "    results = asyncio.run(\n",
    "        gather_abstract_relevance(retrieved_abstracts, NUM_ABSTRACTS_TO_SCORE)\n",
    "    )\n",
    "\n",
    "print(f\"\\n✓ Completed scoring {len(results)} abstracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "scoring-stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Scoring Statistics:\n",
      "  - Papers scored: 3\n",
      "  - Mean score: 78.33\n",
      "  - Std dev: 9.43\n",
      "  - Min score: 65.00\n",
      "  - Max score: 85.00\n",
      "  - Median score: 85.00\n",
      "\n",
      "Sample relevance assessments:\n",
      "\n",
      "  Paper ID 34 (Score: 85.00):\n",
      "    For: The candidate paper explores the application of Retrieval-Augmented Generation (RAG) pipelines speci...\n",
      "    Against: While the candidate paper offers insights into RAG in biomedical contexts, it does not propose a new...\n",
      "\n",
      "  Paper ID 1 (Score: 85.00):\n",
      "    For: The candidate paper directly addresses Retrieval-Augmented Generation (RAG) systems applied to scien...\n",
      "    Against: While the candidate paper discusses RAG in relation to general scientific literature, it does not sp...\n",
      "\n",
      "  Paper ID 50 (Score: 65.00):\n",
      "    For: The candidate reference abstract discusses the importance of accessing biomedical literature and hig...\n",
      "    Against: The reference abstract does not specifically address retrieval-augmented generation (RAG) or its app...\n"
     ]
    }
   ],
   "source": [
    "# Display scoring statistics\n",
    "scores = [abs.probability_score for abs in results]\n",
    "\n",
    "print(\"Relevance Scoring Statistics:\")\n",
    "print(f\"  - Papers scored: {len(scores)}\")\n",
    "print(f\"  - Mean score: {np.mean(scores):.2f}\")\n",
    "print(f\"  - Std dev: {np.std(scores):.2f}\")\n",
    "print(f\"  - Min score: {np.min(scores):.2f}\")\n",
    "print(f\"  - Max score: {np.max(scores):.2f}\")\n",
    "print(f\"  - Median score: {np.median(scores):.2f}\")\n",
    "\n",
    "# Show sample of results\n",
    "print(f\"\\nSample relevance assessments:\")\n",
    "for i, result in enumerate(results[:3]):\n",
    "    print(f\"\\n  Paper ID {result.id} (Score: {result.probability_score:.2f}):\")\n",
    "    print(f\"    For: {result.arguments_for[:100]}...\")\n",
    "    print(f\"    Against: {result.arguments_against[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selection-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Top-K Selection\n",
    "\n",
    "Select the top-k most relevant papers based on their probability scores. These papers will be used to generate the \"Related Work\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "topk-function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Selected top 3 papers by relevance score\n"
     ]
    }
   ],
   "source": [
    "def get_top_k_abstracts(results: List[AbstractRelevance], k: int = 10) -> List[tuple]:\n",
    "    \"\"\"Select top-k papers by relevance score.\n",
    "    \n",
    "    Args:\n",
    "        results: List of AbstractRelevance objects\n",
    "        k: Number of top papers to select\n",
    "    \n",
    "    Returns:\n",
    "        List of (id, score) tuples sorted by score descending\n",
    "    \"\"\"\n",
    "    scores = [(abs.id, abs.probability_score) for abs in results]\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_scores[:k]\n",
    "\n",
    "# Get top-k papers\n",
    "top_k_scores = get_top_k_abstracts(results, k=TOP_K_PAPERS)\n",
    "\n",
    "print(f\"✓ Selected top {TOP_K_PAPERS} papers by relevance score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "display-topk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Papers Selected for Related Work:\n",
      "================================================================================\n",
      "\n",
      "[34] Score: 85.00\n",
      "Title: Reshaping Biomedical Scientific Literature in a RAG Pipeline for Question Answering\n",
      "Abstract: Biomedical Question Answering (BQA) poses specific challenges due to the specialized vocabulary and complex semantic structures of biomedical literature. Large Language Models (LLMs) have shown great ...\n",
      "\n",
      "[1] Score: 85.00\n",
      "Title: PaperQA: Retrieval-Augmented Generative Agent for Scientific Research\n",
      "Abstract: Large Language Models (LLMs) generalize well across language tasks, but suffer from hallucinations and uninterpretability, making it difficult to assess their accuracy without ground-truth. Retrieval-...\n",
      "\n",
      "[50] Score: 65.00\n",
      "Title: Accessing Biomedical Literature in the Current Information Landscape\n",
      "Abstract: Biomedical and life sciences literature is unique because of its exponentially increasing volume and interdisciplinary nature. Biomedical literature access is essential for several types of users incl...\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Reshaping Biomedical Scientific Literature in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>PaperQA: Retrieval-Augmented Generative Agent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Accessing Biomedical Literature in the Current...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  relevance_score                                              title\n",
       "33  34             85.0  Reshaping Biomedical Scientific Literature in ...\n",
       "0    1             85.0  PaperQA: Retrieval-Augmented Generative Agent ...\n",
       "49  50             65.0  Accessing Biomedical Literature in the Current..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract top-k paper IDs and get full information\n",
    "top_k_id = [id for id, score in top_k_scores]\n",
    "top_k_abstracts = retrieved_abstracts[retrieved_abstracts['id'].isin(top_k_id)].copy()\n",
    "\n",
    "# Add scores to DataFrame for display\n",
    "score_dict = {id: score for id, score in top_k_scores}\n",
    "top_k_abstracts['relevance_score'] = top_k_abstracts['id'].map(score_dict)\n",
    "top_k_abstracts = top_k_abstracts.sort_values('relevance_score', ascending=False)\n",
    "\n",
    "print(f\"Top {TOP_K_PAPERS} Papers Selected for Related Work:\")\n",
    "print(\"=\" * 80)\n",
    "for idx, row in top_k_abstracts.iterrows():\n",
    "    print(f\"\\n[{row['id']}] Score: {row['relevance_score']:.2f}\")\n",
    "    print(f\"Title: {row['title']}\")\n",
    "    print(f\"Abstract: {row['abstract'][:200]}...\")\n",
    "\n",
    "# Display as DataFrame\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "display(top_k_abstracts[['id', 'relevance_score', 'title']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generation-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Related Work Generation\n",
    "\n",
    "Generate a cohesive \"Related Work\" section using the top-k papers. The generation agent:\n",
    "- Creates a coherent narrative connecting the papers\n",
    "- Performs critical analysis comparing strengths and weaknesses\n",
    "- Motivates the proposed approach in context of prior work\n",
    "- Cites papers using [id] format\n",
    "- Avoids copying abstracts verbatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "generation-prompt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generation instructions defined\n"
     ]
    }
   ],
   "source": [
    "# Define instructions for related work generation\n",
    "INSTRUCTIONS_RELATED_WORK = \"\"\" \n",
    "You are an expert research assistant who is helping with literature review for a research idea or abstract. \n",
    "You will be provided with an abstract or research idea and a list of reference abstracts. \n",
    "Your task is to write the related work section of the document using only the provided reference abstracts. \n",
    "Please write the related work section creating a cohesive storyline by doing a critical analysis of prior work \n",
    "in the reference abstracts comparing the strengths and weaknesses while also motivating the proposed approach. \n",
    "You should cite the reference abstracts as [id] whenever you are referring it in the related work. \n",
    "Do not write it as Reference #. Do not cite abstract or research Idea. \n",
    "Do not include any extra notes or newline characters at the end. \n",
    "Do not copy the abstracts of reference papers directly but compare and contrast to the main work concisely. \n",
    "Do not provide the output in bullet points or markdown. \n",
    "Do not provide references at the end. \n",
    "Please cite all the provided reference papers if needed.\n",
    "\"\"\"\n",
    "\n",
    "print(\"✓ Generation instructions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "build-generation-input",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Built generation input (4758 characters)\n"
     ]
    }
   ],
   "source": [
    "# Build input for related work generation\n",
    "input_related_work = f\"Given the Research Idea or abstract: {query}\"\n",
    "input_related_work += \"\\n\\n## Given references abstracts list below:\"\n",
    "\n",
    "for index, item in top_k_abstracts[['id', 'title_abstract']].iterrows():\n",
    "    input_related_work += f\"\\n\\n[{item['id']}]: {item['title_abstract']}\"\n",
    "\n",
    "input_related_work += \"\\n\\nWrite the related work section summarizing in a cohesive story prior works relevant to the research idea.\"\n",
    "input_related_work += \"\\n\\n## Related Work:\"\n",
    "\n",
    "print(f\"✓ Built generation input ({len(input_related_work)} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "generate-related-work",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Related work section generated\n",
      "  Length: 2767 characters\n",
      "  Words: ~375 words\n",
      "CPU times: user 7.35 ms, sys: 2.7 ms, total: 10 ms\n",
      "Wall time: 8.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Generate related work section\n",
    "response = openai_client.responses.create(\n",
    "    model=GENERATION_MODEL,\n",
    "    instructions=INSTRUCTIONS_RELATED_WORK,\n",
    "    input=input_related_work\n",
    ")\n",
    "\n",
    "generated_related_work = response.output_text\n",
    "\n",
    "print(\"✓ Related work section generated\")\n",
    "print(f\"  Length: {len(generated_related_work)} characters\")\n",
    "print(f\"  Words: ~{len(generated_related_work.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Results & Evaluation\n",
    "\n",
    "Display the final generated \"Related Work\" section with formatting and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "display-results",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Generated Related Work Section"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Retrieval-augmented generation (RAG) systems have gained traction in addressing the complexities associated with biomedical literature, yet their efficacy in this domain remains inconsistent. Prior work highlights the necessity of combining retrieval methods with large language models (LLMs) to improve question-answering capabilities in biomedical contexts. For example, one study delves into the challenges presented by the unique vocabulary and semantic intricacies of biomedical literature, revealing that RAG frameworks can significantly enhance the performance of LLMs by structuring the context more effectively. This approach not only boosts the quality of generated responses but also emphasizes the importance of precision over recall in generating accurate answers [34].\n",
       "\n",
       "Conversely, another investigation, PaperQA, showcases a RAG agent specifically designed for scientific research. This framework exemplifies how integrating retrieval capabilities with LLMs can mitigate issues related to hallucinations and enhance the interpretability of generated content. By leveraging full-text scientific articles and employing a more complex benchmarking system, PaperQA surpasses existing models in performance, aligning closer to human-like research methodologies [1]. However, while these advancements are notable, they underscore the ongoing challenges surrounding grounding in factual information and the need for robust evaluation metrics.\n",
       "\n",
       "In the broader context of biomedical literature access, significant efforts have been made to develop search tools that cater to various user needs, including researchers and clinicians. Notable systems and tools, such as PubMed and Google Scholar, facilitate access to an expanding volume of literature but often fall short in terms of precise query formulation and result interpretation [50]. This highlights a gap in methodologies that leverage RAG approaches to streamline not just the retrieval but also the synthesis of knowledge, thereby making biomedical literature more accessible to a wider audience.\n",
       "\n",
       "The interactions between retrieval methodologies and LLMs place emphasis on the need for innovative strategies that can bridge existing knowledge gaps and improve communication of complex information in a more digestible format. As the landscape of biomedical literature continues to evolve, the proposed advancements in RAG systems for question answering present a pivotal opportunity to both enhance research accessibility and improve the user experience in navigating scientific information. This context sets the stage for the proposed strategy to optimize RAG methodologies for biomedical applications, ensuring that the dissemination of knowledge aligns with public health needs and understanding."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the generated related work section\n",
    "display(Markdown(\"## Generated Related Work Section\"))\n",
    "display(Markdown(\"---\"))\n",
    "display(Markdown(generated_related_work))\n",
    "display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "citations-used",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citations Used in Generated Text:\n",
      "  - Total citations: 3\n",
      "  - Unique papers cited: 3\n",
      "  - Papers provided: 3\n",
      "  - Citation IDs: [1, 34, 50]\n",
      "\n",
      "Cited Papers:\n",
      "  [1] PaperQA: Retrieval-Augmented Generative Agent for Scientific Research\n",
      "  [34] Reshaping Biomedical Scientific Literature in a RAG Pipeline for Question Answering\n",
      "  [50] Accessing Biomedical Literature in the Current Information Landscape\n"
     ]
    }
   ],
   "source": [
    "# Extract citations used in the generated text\n",
    "import re\n",
    "\n",
    "citations = re.findall(r'\\[(\\d+)\\]', generated_related_work)\n",
    "unique_citations = sorted(set(int(c) for c in citations))\n",
    "\n",
    "print(f\"Citations Used in Generated Text:\")\n",
    "print(f\"  - Total citations: {len(citations)}\")\n",
    "print(f\"  - Unique papers cited: {len(unique_citations)}\")\n",
    "print(f\"  - Papers provided: {len(top_k_abstracts)}\")\n",
    "print(f\"  - Citation IDs: {unique_citations}\")\n",
    "\n",
    "# Show which papers were cited\n",
    "print(f\"\\nCited Papers:\")\n",
    "for paper_id in unique_citations:\n",
    "    paper = top_k_abstracts[top_k_abstracts['id'] == paper_id]\n",
    "    if not paper.empty:\n",
    "        print(f\"  [{paper_id}] {paper.iloc[0]['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "pipeline-summary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Pipeline Execution Summary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Configuration:**\n",
       "- Corpus size: 78 papers\n",
       "- Hybrid retrieval: Top 50 papers\n",
       "- Papers retrieved: 23 unique papers\n",
       "- Papers scored: 3 papers\n",
       "- Top-K selection: 3 papers\n",
       "- Papers cited in output: 3 papers\n",
       "\n",
       "**Models Used:**\n",
       "- Relevance scoring: gpt-4o-mini\n",
       "- Related work generation: gpt-4o-mini\n",
       "\n",
       "**Output:**\n",
       "- Related work length: 2767 characters (~375 words)\n",
       "- Citations included: 3 total, 3 unique\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pipeline execution summary\n",
    "display(Markdown(\"## Pipeline Execution Summary\"))\n",
    "\n",
    "summary = f\"\"\"\n",
    "**Configuration:**\n",
    "- Corpus size: {len(all_abstracts)} papers\n",
    "- Hybrid retrieval: Top {HYBRID_SEARCH_K} papers\n",
    "- Papers retrieved: {len(retrieved_abstracts)} unique papers\n",
    "- Papers scored: {len(results)} papers\n",
    "- Top-K selection: {TOP_K_PAPERS} papers\n",
    "- Papers cited in output: {len(unique_citations)} papers\n",
    "\n",
    "**Models Used:**\n",
    "- Relevance scoring: {RELEVANCE_MODEL}\n",
    "- Related work generation: {GENERATION_MODEL}\n",
    "\n",
    "**Output:**\n",
    "- Related work length: {len(generated_related_work)} characters (~{len(generated_related_work.split())} words)\n",
    "- Citations included: {len(citations)} total, {len(unique_citations)} unique\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "save-output",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Output saved to generated_related_work.txt\n"
     ]
    }
   ],
   "source": [
    "# Optional: Save the generated related work to a file\n",
    "SAVE_OUTPUT = True  # Set to True to save\n",
    "\n",
    "if SAVE_OUTPUT:\n",
    "    output_file = \"generated_related_work.txt\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"RESEARCH QUERY:\\n\")\n",
    "        f.write(query)\n",
    "        f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "        f.write(\"RELATED WORK:\\n\")\n",
    "        f.write(generated_related_work)\n",
    "        f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "        f.write(\"REFERENCES:\\n\")\n",
    "        for paper_id in unique_citations:\n",
    "            paper = top_k_abstracts[top_k_abstracts['id'] == paper_id]\n",
    "            if not paper.empty:\n",
    "                f.write(f\"[{paper_id}] {paper.iloc[0]['title']}\\n\")\n",
    "    \n",
    "    print(f\"✓ Output saved to {output_file}\")\n",
    "else:\n",
    "    print(\"Output not saved (set SAVE_OUTPUT=True to save)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated an end-to-end agentic RAG pipeline for automated literature review generation. Key features:\n",
    "\n",
    "1. **Hybrid Retrieval**: Combines semantic and keyword search for comprehensive coverage\n",
    "2. **Agentic Scoring**: Uses structured reasoning (debate-style) for reliable relevance assessment\n",
    "3. **Parallel Processing**: Efficiently scores multiple papers concurrently\n",
    "4. **Coherent Synthesis**: Generates well-structured literature reviews with proper citations\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with different retrieval parameters (`HYBRID_SEARCH_K`)\n",
    "- Adjust the number of papers to score (`NUM_ABSTRACTS_TO_SCORE`)\n",
    "- Try different top-k values (`TOP_K_PAPERS`)\n",
    "- Evaluate different LLM models for scoring and generation\n",
    "- Expand the corpus with more biomedical abstracts\n",
    "- Add evaluation metrics for generated related work quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-lit-llm (3.13.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
